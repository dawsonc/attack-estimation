{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Networks for Estimating Attack Vector\n",
    "\n",
    "## Intro & motivating example\n",
    "- High-level overview of what this tutorial covers (i.e. a plain-language description: \"Bayesian networks offer a way to model causality in the language of probabilities, providing a more compact representation of the probabilistic relationships between variables\")\n",
    "- Motivating star wars^tm example from project description\n",
    "- More detailed list of topics/methods covered and how they relate to the example. i.e. give an outline of the rest of the tutorial.\n",
    "\n",
    "### References\n",
    "- \\[AIMA\\] Chapter 13, 14\n",
    "- [R. Dechter, \"Bucket Elimination: A Unifying Framework for Probabilistic Inference\"](https://webdocs.cs.ualberta.ca/~rgreiner/C-366/RG-2002-SLIDES/BucketElim.pdf) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability Review\n",
    "- This section won't provide a huge overview of probability, but just a reminder of the relevant parts.\n",
    "- Random variables and joint probability distributions (for n binary variables, requires 2^n storage)\n",
    "- Reasoning using joint distributions (13.3)\n",
    "    - just read out e.g. P(A ^ B)\n",
    "    - marginalization to get marginal probability e.g. just P(A) = P(A ^ B) + P(A ^ !B) (plus general eq.)\n",
    "    - Conditional/posterior probabilities, product rule P(A ^ B) = P(A | B)P(B), and conditioning (marginalization but with conditional probs).\n",
    "    - Derive P(X|e) = P(X^e)/P(e) = aP(X^e) = a \\sum_y P(X, e, y) (normalization to get a so that P(X|e)+P(!X|e)=1\n",
    "    - Remark: this would allow us to answer whatever queries we want to ask just by looking in the joint distribution table. BUT that table has size O(2^n) and requires O(2^n) to process by summing over it.\n",
    "- Reasoning using conditional distributions (13.4)\n",
    "    - Lots of redundant information in full joint distribution (like how your toothache affects the weather, ignoring causality)\n",
    "    - Independence! Conditional independence (based on causality)!\n",
    "    - Allows factoring the joint distribution into multiple smaller conditional distributions, saving on size. 2^n+2^m << 2^{n+m}    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Networks\n",
    "- Smooth segue: how do we formalize these conditional distributions into an easy-to-use data structure?\n",
    "- What is a Bayesian network\n",
    "    - Define the data structure (p511)\n",
    "    - Remark on how it encodes causality via conditional independence (siblings conditionally independent of each other) and independence (unconnected networks are independent).\n",
    "- Simple example\n",
    "    - How to construct theoretically\n",
    "    - How to construct with the python class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example code for constructing the simple burglary example from AIMA (Fig. 14.2) goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What can they do for you (with reference to the simple example)?\n",
    "    - Answer queries on the joint probability distribution e.g. P(a, b, !c, d, !e) = \\product_i P(x_i | parents(x_i))\n",
    "    - Query the posterior given evidence e.g. P(X | e) = aP(X, e) = a \\sum_{y not x not e} P(X, e, y) <- same as above\n",
    "    - MAP: max liklihood explanation, the X with the highest probability given e."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brute Force Method\n",
    "- Describe brute-force method for getting probability estimates\n",
    "    - 14.4.1\n",
    "    - O(n2^n) for n vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def brute_force_query(X, e, net):\n",
    "    \"\"\"Returns the posterior probability P(x | e) using the Bayesian network `net`,\n",
    "    using a brute force enumeration method with time complexity O(n2^n), where\n",
    "    n is the number of variables in the network.\n",
    "    \n",
    "    Inputs:\n",
    "        X: a list of variable names for which we will calculate posteriors\n",
    "        e: a list of tuples (name, value) specifying assignments to evidence variables\n",
    "            (we assume that e and X are disjoint)\n",
    "    Outputs:\n",
    "        P: a distribution over the query variables in X (a dict mapping names in\n",
    "           X to a probability value)\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- benchmark the brute force algorithm to show how exponential it is\n",
    "    - Simple example from last section\n",
    "    - Star wars example from this section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run brute_force_query on the two examples to see how long they take"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Simple speedup via DFS enumeration-ask alg from book fig 14.9\n",
    "    - Explain how it works\n",
    "    - Still super slow: O(2^n) for n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CITE: Russel & Norvig AIMA Ch 14, Section 4, Fig. 14.9\n",
    "def simple_query(X, e, net):\n",
    "    \"\"\"Returns the posterior probability P(x | e) using the Bayesian network `net`,\n",
    "    using an improved enumeration method with time complexity O(2^n), where n is\n",
    "    the number of variables in the network, using analogue to depth-first search.\n",
    "    \n",
    "    Inputs:\n",
    "        X: a list of variable names for which we will calculate posteriors\n",
    "        e: a list of tuples (name, value) specifying assignments to evidence variables\n",
    "            (we assume that e and X are disjoint)\n",
    "    Outputs:\n",
    "        P: a distribution over the query variables in X (a dict mapping names in\n",
    "           X to a probability value)\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "def enumerate_all(variables, e):\n",
    "    \"\"\"Computes one step in the depth-first iteration of the Bayesian network\"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- benchmark the simple algorithm to show how it performs better than the brute force method\n",
    "    - Simple example from last section\n",
    "    - Star wars example from this section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run simple_force_query on the two examples to see how long they take"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bucket Elimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the main notebook to run.\n",
    "# In this notebook, we go through the \"attack-estimation\" portion of the project.\n",
    "# Some utility code, including base classes, lives in files outside this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A bunch of imports\n",
    "from bayes_net import BayesNet, BayesNode\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.024000000000000004\n",
      "Have 4 nodes and 3 edges\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rain\n",
      "Sprinklers\n",
      "Grass\n",
      "Feet\n"
     ]
    }
   ],
   "source": [
    "# A demonstration for developers of how to create a BayesNet. In this example, I define the likelihood of getting\n",
    "# wet feet as a function of wet grass, which is a function of raining and sprinkling.\n",
    "\n",
    "# First, declare a bunch of nodes.\n",
    "rain_node = BayesNode('Rain')\n",
    "rain_node.set_marginal_distribution({True: 0.2, False: 0.8})\n",
    "sprinklers_node = BayesNode('Sprinklers')\n",
    "sprinklers_node.set_marginal_distribution({'on': 0.6, 'off': 0.4})\n",
    "grass_node = BayesNode('Grass')\n",
    "grass_node.add_entry([(rain_node, True), (sprinklers_node, 'on')], {'wet': 0.95, 'dry': 0.05})\n",
    "grass_node.add_entry([(rain_node, True), (sprinklers_node, 'off')], {'wet': 0.6, 'dry': 0.4})\n",
    "grass_node.add_entry([(rain_node, False), (sprinklers_node, 'on')], {'wet': 0.45, 'dry': 0.55})\n",
    "grass_node.add_entry([(rain_node, False), (sprinklers_node, 'off')], {'wet': 0.1, 'dry': 0.90})\n",
    "feet_node = BayesNode('Feet')\n",
    "feet_node.add_entry([(grass_node, 'wet')], {'dry': 0.1, 'damp': 0.5, 'drenched': 0.4})\n",
    "feet_node.add_entry([(grass_node, 'dry')], {'dry': 0.7, 'damp': 0.2, 'drenched': 0.1})\n",
    "\n",
    "# Second, create a BayesNet object that just stores all the nodes.\n",
    "net = BayesNet([rain_node, sprinklers_node, grass_node, feet_node])\n",
    "\n",
    "# Third, do whatever you want with this data structure, like ask for the conditional distribution for a variable.\n",
    "fetched_node = net.get_node('Feet')\n",
    "assert fetched_node == feet_node  # Just a sanity check\n",
    "# Calculate some joint probabilities\n",
    "joint_prob = net.calc_joint([(rain_node, True), (sprinklers_node, 'off'), (feet_node, 'damp'), (grass_node, 'wet')])\n",
    "print(joint_prob)\n",
    "\n",
    "# Fourth, visualize it all. Right now, visualization is crude (weird layout) but should be correct (arrows the right way.)\n",
    "net.draw_net()\n",
    "\n",
    "# Fifth, show off a fancy new topographical ordering method I just wrote.\n",
    "ordered_nodes = net.get_topographical_ordering()\n",
    "for node in ordered_nodes:\n",
    "    print(node.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Approximate Inference:\n",
    "\n",
    "So far in this notebook, we've stuck with relatively small nets and simple distributions. That means that doing exact inference - calculating analytically exactly what some distribution will look like - is possible. For a lot of problems that we care about, though, exact inference isn't possible. There are lots of reasons this might happen: some distribution is wonky and therefore can't be reasoned about analytically, a net is so complex that doing all the math to marginalize out variables seems impossible, etc.\n",
    "\n",
    "But we don't have to give up. In the following examples, we'll implement two sorts of approximate inference techniques: rejection sampling and Gibbs sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matches_evidence(node, value, evidence):\n",
    "    for evidence_name, evidence_value in evidence:\n",
    "        if node.name == evidence_name:\n",
    "            if value == evidence_value:\n",
    "                return True\n",
    "            return False\n",
    "    return True\n",
    "    \n",
    "# In this sampling method, we just generate random setups through sampling.\n",
    "# If we satisfy the evidence, we save the state; if we don't we reject the sample and keep trying.\n",
    "# After saving lots of states, the hope is that we have enough counts to estimate the frequencies of other variables\n",
    "# conditioned on the evidence.\n",
    "def rejection_sampling(X, e, net, num_samples=10000):\n",
    "    # For each of the variables in X, store a count of how often each value in the domain appears.\n",
    "    # Intialization to zero counts everywhere.\n",
    "    var_to_val_to_count = {}\n",
    "    for x in X:\n",
    "        var_to_val_to_count[x] = {}\n",
    "        for val in net.get_node(x).domain:\n",
    "            var_to_val_to_count[x][val] = 0\n",
    "    # Now, just generate tons of samples in the net, rejecting if it doesn't match the evidence.\n",
    "    num_rejects = 0\n",
    "    for sample_idx in range(num_samples):\n",
    "        # Get a topographical ordering to start sampling.\n",
    "        ordered_nodes = net.get_topographical_ordering()\n",
    "        assignments = {}\n",
    "        reject_sample = False\n",
    "        for node in ordered_nodes:\n",
    "            if node.marginal_distribution:\n",
    "                sample = node.draw_sample()\n",
    "                assignments[node] = sample\n",
    "            else:\n",
    "                parent_val_assignments = [(parent, assignments.get(parent)) for parent in node.parents]\n",
    "                sample = node.draw_sample(parent_vals=parent_val_assignments)\n",
    "                assignments[node] = sample\n",
    "            # Do the rejection part if the node that was sampled contradicts the evidence\n",
    "            if not matches_evidence(node, sample, e):\n",
    "                reject_sample = True\n",
    "                break\n",
    "        if reject_sample:\n",
    "            num_rejects += 1\n",
    "            continue\n",
    "        # Matched the evidence, so update the counts of valid variable assignments\n",
    "        for assigned_node, assigned_val in assignments.items():\n",
    "            if assigned_node.name in var_to_val_to_count.keys():\n",
    "                var_to_val_to_count[assigned_node.name][assigned_val] += 1\n",
    "    # At the end, finally have counts that we can use to compute probabilities.\n",
    "    for x in X:\n",
    "        relevant_counts = var_to_val_to_count.get(x)\n",
    "        total_count = sum([count for count in relevant_counts.values()])\n",
    "        normalized_distribution = {}\n",
    "        for value, count in relevant_counts.items():\n",
    "            normalized_distribution[value] = count / total_count\n",
    "        print(\"Distribution for \", x, \":\", normalized_distribution)\n",
    "    print(\"Num samples used:\", num_samples - num_rejects)\n",
    "    print(\"Num samples rejected\", num_rejects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution for  Feet : {'dry': 0.5102168735113451, 'damp': 0.30036354519242825, 'drenched': 0.18941958129622666}\n",
      "Num samples used: 7977\n",
      "Num samples rejected 2023\n",
      "Distribution for  Rain : {True: 0.2998102466793169, False: 0.7001897533206831}\n",
      "Distribution for  Grass : {'wet': 0.8285895003162556, 'dry': 0.17141049968374447}\n",
      "Num samples used: 1581\n",
      "Num samples rejected 8419\n"
     ]
    }
   ],
   "source": [
    "# Test the rejection sampling code.\n",
    "# First, a really simple example.\n",
    "rejection_sampling(['Feet'], [('Rain', False)], net)\n",
    "# Now, a harder one, with more evidence and asking about more\n",
    "rejection_sampling(['Rain', 'Grass'], [('Sprinklers', 'on'), ('Feet', 'drenched')], net)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous example, we saw how rejection sampling can get us the distributions we want, but we also see one of the big flaws: we end up rejecting lots of samples. Lets do gibbs sampling instead!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXME: this code is ugly, but I do think it's correct.\n",
    "def gibbs_sampling(X, e, net, burn_in_period=100, eval_period=5000):\n",
    "    # For each of the variables in X, store a count of how often each value in the domain appears.\n",
    "    # Intialization to zero counts everywhere.\n",
    "    var_to_val_to_count = {}\n",
    "    for x in X:\n",
    "        var_to_val_to_count[x] = {}\n",
    "        for val in net.get_node(x).domain:\n",
    "            var_to_val_to_count[x][val] = 0\n",
    "    evidence_names = [evidence[0] for evidence in e]\n",
    "    # Intialize the net with random assignments.\n",
    "    assignments = {}\n",
    "    ordered_nodes = net.get_topographical_ordering()\n",
    "    for node in ordered_nodes:\n",
    "        if node.marginal_distribution:\n",
    "            sample = node.draw_sample()\n",
    "            assignments[node] = sample\n",
    "        else:\n",
    "            parent_val_assignments = [(parent, assignments.get(parent)) for parent in node.parents]\n",
    "            sample = node.draw_sample(parent_vals=parent_val_assignments)\n",
    "            assignments[node] = sample\n",
    "    # Now the burn-in section\n",
    "    all_nodes = net.nodes\n",
    "    for trial in range(burn_in_period + eval_period):\n",
    "        # Choose a random node, as long as it's not evidence.\n",
    "        node_to_swap = np.random.choice(all_nodes)\n",
    "        while node_to_swap.name in evidence_names:\n",
    "            node_to_swap = np.random.choice(all_nodes)\n",
    "        # Generate the distribution over next possible values of the node, conditioned on parents and children.\n",
    "        parent_assignments = [(parent, assignments.get(parent)) for parent in node_to_swap.parents]\n",
    "        next_distribution = {}\n",
    "        for next_val in node_to_swap.domain:\n",
    "            prob_given_parents = node_to_swap.get_prob_value(next_val, parent_assignments)\n",
    "            children = net.get_children(node_to_swap)\n",
    "            prob_from_children = 1.0\n",
    "            for child in children:\n",
    "                childs_parent_assignments = []\n",
    "                # But overwrite with next_val\n",
    "                for parent in child.parents:\n",
    "                    if parent == node_to_swap:\n",
    "                        childs_parent_assignments.append((node_to_swap, next_val))\n",
    "                        continue\n",
    "                    childs_parent_assignments.append((parent, assignments.get(parent)))\n",
    "                prob_of_child = child.get_prob_value(assignments.get(child), childs_parent_assignments)\n",
    "                prob_from_children = prob_from_children * prob_of_child\n",
    "            total_prob = prob_given_parents * prob_from_children\n",
    "            next_distribution[next_val] = total_prob\n",
    "        # Normalize the distribution and then sample.\n",
    "        normalizing_factor = sum(next_distribution.values())\n",
    "        for entry, val in next_distribution.items():\n",
    "            next_distribution[entry] = val / normalizing_factor\n",
    "        values = [entry[0] for entry in sorted(next_distribution.items())]\n",
    "        probabilities = [entry[1] for entry in sorted(next_distribution.items())]\n",
    "        sampled = np.random.choice(values, p=probabilities)\n",
    "        assignments[node_to_swap] = sampled\n",
    "        # If after burn-in, start saving data\n",
    "        if trial <= burn_in_period:\n",
    "            continue\n",
    "        # Save the data.\n",
    "        for assigned_node, assigned_val in assignments.items():\n",
    "            if assigned_node.name in var_to_val_to_count.keys():\n",
    "                var_to_val_to_count[assigned_node.name][assigned_val] += 1\n",
    "    # At the end, finally have counts that we can use to compute probabilities.\n",
    "    for x in X:\n",
    "        relevant_counts = var_to_val_to_count.get(x)\n",
    "        total_count = sum([count for count in relevant_counts.values()])\n",
    "        normalized_distribution = {}\n",
    "        for value, count in relevant_counts.items():\n",
    "            normalized_distribution[value] = count / total_count\n",
    "        print(\"Distribution for \", x, \":\", normalized_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution for  Feet : {'dry': 0.5471094218843768, 'damp': 0.27345469093818764, 'drenched': 0.1794358871774355}\n",
      "\n",
      "Distribution for  Rain : {True: 0.27825565113022604, False: 0.7217443488697739}\n",
      "Distribution for  Grass : {'wet': 0.8195639127825565, 'dry': 0.18043608721744347}\n"
     ]
    }
   ],
   "source": [
    "# Test out Gibbs with the same examples as for rejection sampling\n",
    "gibbs_sampling(['Feet'], [('Rain', False)], net)\n",
    "print()\n",
    "gibbs_sampling(['Rain', 'Grass'], [('Sprinklers', 'on'), ('Feet', 'drenched')], net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
