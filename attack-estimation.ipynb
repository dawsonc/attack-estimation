{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Networks for Estimating Attack Vector\n",
    "\n",
    "## Intro & motivating example\n",
    "\n",
    "Humans are notoriously bad at reasoning about probabilities: more often than not, we rely on cognitive shortcuts (heuristics and biases) instead of actually calculating the liklihood of events. This can be a good thing; just think how paralyzed we would be if we stopped to calculate the probability of rain every time we heard raindrops on our window, rather than just grabbing an umbrella! Even computers can start to run into tractability problems when working with probability distributions involving even a few dozen variables. In this tutorial, we will introduce a tool, called Bayesian networks, for automatically and efficiently reasoning about probabilities. These networks will allow us to ask questions like \"given this new evidence, what is the probability of some hidden variable being true\" or \"what is the most likely explanation for some set of observations?\". These questions might remind you of our lectures on Hidden Markov Models (HMMs); it turns out that Bayesian networks can be used to represent Markov processes, but they are much more expressive and can be used to model much more complicated scenarios.\n",
    "\n",
    "To make this discussion more concrete, it's helpful to have a motivating example. You are a security engineer in the employ of the Generic Galactic Empire aboard a controversial new ~~moon~~ space station. You are aware of several security vulnerabilities in the space station's software subsystems, but you have no way to detect whether those vulnerabilities are being exploited except by running diagnostics on various workstations throughout the station. Since you'd like to be able to detect attacks based on the probability that a vulnerability is being exploited, you'd like an easy way to relate your observations of workstations to the probability of a cyberattack taking place. After a brief review of probability fundamentals, we'll discuss how Bayesian networks can be used to model this situation and make inferences automatically.\n",
    "\n",
    "### References\n",
    "The discussion in this tutorial draws heavily from the following sources:\n",
    "- Russell & Norvig, AIMA, Chapter 13 & 14\n",
    "- [R. Dechter, \"Bucket Elimination: A Unifying Framework for Probabilistic Inference\"](https://webdocs.cs.ualberta.ca/~rgreiner/C-366/RG-2002-SLIDES/BucketElim.pdf) \n",
    "\n",
    "Chapter 13 of AIMA provides an overview of probability fundamentals, while chapter 14 discusses Bayesian networks in more depth. Dechter's article describes efficient algorithms for making inference based on Bayesian networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability Review\n",
    "\n",
    "This section will provide a quick review of relevant probability concepts. For a more thorough review, see Russel & Norvig AIMA, Chapter 13.\n",
    "\n",
    "Given a set of $n$ discrete random variables, we can fully represent the probabilities and relationships between all $n$ variables using a table called the **joint distribution**. This table has a dimension for every variable and a row for each possible assignment to that variable; a joint distribution for 2 binary variables, $A$ and $B$, would have a row for each value of $A$ and a column for each value of $B$. Each entry in the joint distribution contains the probability that the corresponding assignment to each of the variables will be observed; in the $2\\times2$ case, the $(0, 0)$ entry would contain the probability that $A=0$ and $B=0$.\n",
    "\n",
    "Formally, the joint distribution allows us to lookup probabilities $P(A\\land B \\land C \\land ...)$ just be reading the value out of the joint distribution table. They also allow us to find the so-called *marginal probability* of some variable $P(A)$, which is the probability that $A$ takes on some value regardless of the value of all other variables. In the binary case, $P(A) = P(A\\land B) + P(A \\land \\text{not }B)$; in the general case, we can calculate the marginal probability by summing over all possible values of the variables we don't care about (this is called **marginalizing** out those summed variable), like so:\n",
    "\n",
    "$$P(A) = \\sum_{\\text{all possible assignments to }B,C,\\ldots} P(A \\land B \\land C \\land \\ldots$$\n",
    "\n",
    "Often, we are interested in the case when we have observed some variable assignments (which we call the evidence variables $e$), and we are interested in the probability of some other variables ($X$) given those observations. This probability $P(X | e)$ is called the **conditional** or **posterior probability**, and it can be calculated using the product rule, $P(A\\land e) = P(X|e) P(e)$, which we can rearrange to find \n",
    "\n",
    "$$P(X | e) = P(X\\land e)/P(e) = \\alpha P(X \\land e)$$\n",
    "\n",
    "where $\\alpha$ is a normalization factor calculated so that $P(X | e) + P(\\text{not }X | e) = 1$. Note that $P(X \\land e)$ is simply a marginal probability, so we can compute it from the joint distribution by marginalizing:\n",
    "\n",
    "$$P(X\\land e) = \\sum_{\\text{all possible assignments to }y\\text{ not in}x, e}P(X \\land e \\land y)$$\n",
    "\n",
    "\n",
    "Using these equations, we can answer whatever queries we want just by looking up entries in the corresponding joint distribution table. This would be nice, but unfortunately these joint distributions are often too large to use in practice. Even for relatively simple binary variables, the size of the table grows exponentially, $O(2^n)$, where $n$ is the number of variables.\n",
    "\n",
    "To get around this exponential growth, we can apply some common sense. Let's say we want to calculate the probability that three sequential fair coin tosses both come out heads. We could create a joint distribution table for the two variables ($H1$ for whether the first coin comes out heads, and so on for $H2$ and $H3$). This joint table would have $2^3=8$ entries. However, we know that the outcome of the first coin toss does not influence the outcome of later tosses. Formally, we know that $P(H3 | H1, H2) = P(H3)$, and similarly for $P(H1 | H2, H3) = P(H1)$ and $P(H2 | H1, H3) = P(H2)$. Because these variables are **independent**, we only need to specify a distribution for each set of independent variables. In this case, we would need three **conditional distributions**, each with 2 entries (6 total entries, saving 2 compared to the full joint distribution, but these savings grow exponentially). In general, if we have multiple sets of variables, where the variables in each set are independent from variables in other sets, we need one conditional distribution per set. Intuitively, if the joint distribution needs $2^n$ storage, two conditional distributions would only need $2^{n/2}+2^{n/2} \\ll 2^n$. By encoding our common-sense knowledge (e.g. of causality and independence), we can make probabilistic inference much more easily than over the full joint distribution.\n",
    "\n",
    "In the next section, we'll discuss a data structure for making efficient inference with conditional distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Networks\n",
    "\n",
    "Exploiting conditional independence can allow us to reason much more efficiently about joint probabilities, but it would be really nice if we could automate the inference process by encoding conditional probabilities in a convenient data structure. Since dependence relates each variable to a set of \"parent\" variables, a natural structure is a directed graph, where\n",
    "- Each node corresponds to an uncertain variable (in this tutorial, we'll restrict ourselves to discrete random variables),\n",
    "- If variable $B$ is dependent on $A$, then we draw a directed edge from node `A` to node `B` in the graph, and\n",
    "- Each node $X$ is labelled with its conditional probability distribution $P(X\\ |\\ \\text{Parents}(X))$, which takes the form of a table with a row for each combination of assignments to the parent variables of $X$, specifying the probabilities that $X$ takes on each of its values conditioned on those parent variable assignments.\n",
    "\n",
    "Since we don't want any variable to depend (directly or indirectly) on itself, this graph must not have any directed cycles (making it a *directed acyclic graph*. An example graph is shown in the figure below. Given this structure, we can see that it directly encodes both independence and conditional independence: sibling nodes (i.e. nodes sharing a parent) are conditionally independent, while nodes in disconnected graphs are independent.\n",
    "\n",
    "We call this data structure a Bayesian network (or sometimes a *belief network* or *causal network*). These networks can represent any full joint distribution, but depending on the degree to which causal relationships allow us to factor the joint distribution into smaller conditional distributions, Bayesian networks can encode this information much more compactly. In the next example, we'll give a simple example of a Bayesian network and show how we can make inference based on these networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A simple example\n",
    "\n",
    "Before moving on to the more complicated case of our original motivating problem, we'll use the simpler scenario from Fig. 14.2 in Russel & Norvig to illustrate the key features of Bayesian networks. In this scenario, you've just installed a new alarm in your house, and since you don't want Google spying on your home, you've chosen not to connect the alarm to the internet. Instead, you've asked your neighbors, John and Mary, to listen for the alarm and call you if they hear it going off. Complicating matters, the alarm will go off if a burglary occurs, but it is a little too sensitive and will also go off if an earthquake occurs. In addition, John will will sometimes confuse a telephone ringing for the alarm and call then as well, and because Mary likes listening to loud music, she will sometimes miss the alarm altogether.\n",
    "\n",
    "In this scenario, we essentially have five uncertain boolean variables:\n",
    "+ `B`: whether a burglary has occurred,\n",
    "+ `E`: whether an earthquake has occurred,\n",
    "+ `A`: whether the alarm has gone off,\n",
    "+ `J`: whether John calls you, and\n",
    "+ `M`: whether Mary calls you.\n",
    "\n",
    "Note that only `J` and `M` are observable. If we were to encode the full joint probability distribution, we'd need to store a table of size $O(2^n) = O(32)$. This is not too large in this case, but if you have even 30 variables then the joint distribution table would require $O(4$ GB$)$ of storage, and adding a 31st variable would require another 4 GB! We are clearly on the wrong side of this exponential when it comes to the joint distribution.\n",
    "\n",
    "To make the problem more tractable, we can exploit the causal relationships between these variables, allowing us to store a handful of smaller conditional distributions rather than a monolithic joint distribution. The causal structure of this problem leads to the Bayesian network shown in the image below (reprinted from Russell & Norvig):\n",
    "\n",
    "<img src=\"files/figs/alarm-problem-structure.png\">\n",
    "\n",
    "Since `J` and `M` only depend on `A`, we don't need to explicitly store the effect of an earthquake on the probability of John calling. Instead of a 32-entry joint distribution, we only need to store 20 values in 5 smaller conditional distributions. Only 10 entries are shown here because for boolean variables the probability of a false value is fixed at 1-(the probability of a true value). For scenarios with even more variables, these savings would be even more dramatic.\n",
    "\n",
    "It's important to note that the Bayesian network representation is not unique: you can order the variables in any way you want and back out the conditional probability tables from the joint distribution. In this case, we have ordered the nodes from causes to effects (causes at the top, flowing down to effects at the bottom of the network). This is a good heuristic for constructing concise Bayesian networks.\n",
    "\n",
    "We've written classes in `bayes_net.py` implementing the basic functionality of Bayesian networks for you, allowing you to represent these networks programmatically. You should go check out the source code to get an idea of what features are available, and a basic example of constructing a network for this example scenario is given below. To properly load the drawing function, **you must run this cell twice**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Example code for constructing the simple burglary example from AIMA (Fig. 14.2)\n",
    "from bayes_net import BayesNet, BayesNode\n",
    "import numpy as np\n",
    "import warnings\n",
    "# The plotting functionality generates some warnings that aren't too important, so filter them out.\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# First, instantiate the two nodes that have no parents in the network.\n",
    "# The domain of these variables is just {True, False} here, but we support arbitrary discrete domains\n",
    "burglary_node = BayesNode('Burglary')\n",
    "burglary_node.set_marginal_distribution({True: 0.001, False: 0.999})\n",
    "earthquake_node = BayesNode('Earthquake')\n",
    "earthquake_node.set_marginal_distribution({True: 0.002, False: 0.998})\n",
    "\n",
    "# Now we can instantiate nodes with probabilities conditioned on their parents\n",
    "# We have to build the conditional distribution table one entry at a time, for each combination\n",
    "# of parent variables\n",
    "alarm_node = BayesNode('Alarm')\n",
    "alarm_node.add_entry([(burglary_node, True), (earthquake_node, True)], {True: 0.95, False: 0.05})\n",
    "alarm_node.add_entry([(burglary_node, True), (earthquake_node, False)], {True: 0.94, False: 0.06})\n",
    "alarm_node.add_entry([(burglary_node, False), (earthquake_node, True)], {True: 0.29, False: 0.71})\n",
    "alarm_node.add_entry([(burglary_node, False), (earthquake_node, False)], {True: 0.001, False: 0.999})\n",
    "\n",
    "john_node = BayesNode('John')\n",
    "john_node.add_entry([(alarm_node, True)], {True: 0.9, False: 0.1})\n",
    "john_node.add_entry([(alarm_node, False)], {True: 0.05, False: 0.95})\n",
    "\n",
    "mary_node = BayesNode('Mary')\n",
    "mary_node.add_entry([(alarm_node, True)], {True: 0.7, False: 0.3})\n",
    "mary_node.add_entry([(alarm_node, False)], {True: 0.01, False: 0.99})\n",
    "\n",
    "# Now we can create a BayesNet object to store all the nodes.\n",
    "alarm_net = BayesNet([burglary_node, earthquake_node, alarm_node, john_node, mary_node])\n",
    "\n",
    "# As a sanity check, we can visualize the network to make sure it matches our model above.\n",
    "# If the figure doesn't show up the first time, try again.\n",
    "alarm_net.draw_net()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use similar code to construct a more complicated network based on our original motivating example of cyberattack estimation. That situation can be modelled as follows:\n",
    "\n",
    "+ We can directly observe diagnostics on 4 workstations.\n",
    "+ Each workstation depends (to different extents) on 2 unobservable subsystems.\n",
    "+ There are 3 possible attack vectors, each of which affects some combination of subsystems and workstations.\n",
    "\n",
    "+ Each workstation can either be `Up` or `Down`.\n",
    "+ Each subsystem can be either `Online` or `Offline`.\n",
    "+ Each attack vector has either been `Attacked` or else is `Safe`.\n",
    "\n",
    "The code in the cell below constructs a Bayesian network to model this situation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Here's an exmaple net for the example given in the project. The nodes are labelled correctly,\n",
    "# but I have no idea what the appropriate domains or probabilities are.\n",
    "\n",
    "# The top three nodes correspond to possible attacks\n",
    "attack1 = BayesNode('Attack1')\n",
    "attack1.set_marginal_distribution({'Attacked': 0.2, 'Safe': 0.8})\n",
    "attack2 = BayesNode('Attack2')\n",
    "attack2.set_marginal_distribution({'Attacked': 0.3, 'Safe': 0.7})\n",
    "attack3 = BayesNode('Attack3')\n",
    "attack3.set_marginal_distribution({'Attacked': 0.6, 'Safe': 0.4})\n",
    "\n",
    "# The middle two nodes correspond to failing subsystems.\n",
    "subsystem1 = BayesNode('Subsystem1')\n",
    "subsystem1.add_entry([(attack1, 'Attacked'), (attack2, 'Attacked')], {'Offline': 0.5, 'Online': 0.5})\n",
    "subsystem1.add_entry([(attack1, 'Attacked'), (attack2, 'Safe')], {'Offline': 0.4, 'Online': 0.6})\n",
    "subsystem1.add_entry([(attack1, 'Safe'), (attack2, 'Attacked')], {'Offline': 0.3, 'Online': 0.7})\n",
    "subsystem1.add_entry([(attack1, 'Safe'), (attack2, 'Safe')], {'Offline': 0.1, 'Online': 0.9})\n",
    "subsystem2 = BayesNode('Subsystem2')\n",
    "subsystem2.add_entry([(attack1, 'Attacked'), (attack3, 'Attacked')], {'Offline': 0.3, 'Online': 0.7})\n",
    "subsystem2.add_entry([(attack1, 'Attacked'), (attack3, 'Safe')], {'Offline': 0.2, 'Online': 0.8})\n",
    "subsystem2.add_entry([(attack1, 'Safe'), (attack3, 'Attacked')], {'Offline': 0.1, 'Online': 0.9})\n",
    "subsystem2.add_entry([(attack1, 'Safe'), (attack3, 'Safe')], {'Offline': 0.05, 'Online': 0.95})\n",
    "\n",
    "# The bottom four nodes correspond to workstations working or not.\n",
    "workstation1 = BayesNode(\"Workstation1\")\n",
    "workstation1.add_entry([(subsystem1, 'Online')], {'Up': 0.7, 'Down': 0.3})\n",
    "workstation1.add_entry([(subsystem1, 'Offline')], {'Up': 0.3, 'Down': 0.7})\n",
    "workstation2 = BayesNode(\"Workstation2\")\n",
    "workstation2.add_entry([(subsystem1, 'Online'), (subsystem2, 'Online')], {'Up': 0.8, 'Down': 0.2})\n",
    "workstation2.add_entry([(subsystem1, 'Online'), (subsystem2, 'Offline')], {'Up': 0.7, 'Down': 0.3})\n",
    "workstation2.add_entry([(subsystem1, 'Offline'), (subsystem2, 'Online')], {'Up': 0.6, 'Down': 0.4})\n",
    "workstation2.add_entry([(subsystem1, 'Offline'), (subsystem2, 'Offline')], {'Up': 0.2, 'Down': 0.8})\n",
    "workstation3 = BayesNode(\"Workstation3\")\n",
    "workstation3.add_entry([(subsystem1, 'Online'), (subsystem2, 'Online'), (attack2, 'Safe')], {'Up': 0.9, 'Down': 0.1})\n",
    "workstation3.add_entry([(subsystem1, 'Online'), (subsystem2, 'Online'), (attack2, 'Attacked')], {'Up': 0.7, 'Down': 0.3})\n",
    "workstation3.add_entry([(subsystem1, 'Online'), (subsystem2, 'Offline'), (attack2, 'Safe')], {'Up': 0.6, 'Down': 0.4})\n",
    "workstation3.add_entry([(subsystem1, 'Online'), (subsystem2, 'Offline'), (attack2, 'Attacked')], {'Up': 0.3, 'Down': 0.7})\n",
    "workstation3.add_entry([(subsystem1, 'Offline'), (subsystem2, 'Online'), (attack2, 'Safe')], {'Up': 0.7, 'Down': 0.3})\n",
    "workstation3.add_entry([(subsystem1, 'Offline'), (subsystem2, 'Online'), (attack2, 'Attacked')], {'Up': 0.4, 'Down': 0.6})\n",
    "workstation3.add_entry([(subsystem1, 'Offline'), (subsystem2, 'Offline'), (attack2, 'Safe')], {'Up': 0.2, 'Down': 0.8})\n",
    "workstation3.add_entry([(subsystem1, 'Offline'), (subsystem2, 'Offline'), (attack2, 'Attacked')], {'Up': 0.05, 'Down': 0.95})\n",
    "workstation4 = BayesNode(\"Workstation4\")\n",
    "workstation4.add_entry([(subsystem2, 'Online')], {'Up': 0.7, 'Down': 0.3})\n",
    "workstation4.add_entry([(subsystem2, 'Offline')], {'Up': 0.3, 'Down': 0.7})\n",
    "\n",
    "star_net = BayesNet([attack1, attack2, attack3, subsystem1, subsystem2, workstation1, workstation2, workstation3, workstation4])\n",
    "\n",
    "star_net.draw_net()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference on Bayesian Networks\n",
    "\n",
    "Now that we've constructed a bayesian network, what can we do with it? In short, everything you can do with the full joint distribution! In the next few sections, we'll discuss algorithms for computing:\n",
    "- The joint probability distribution e.g. P(a, b, !c, d, !e)\n",
    "- The posterior probability given evidence e.g. P(X | e) = aP(X, e)\n",
    "- The maximum liklihood explanation of a set of evidence, which is the assignments to X with the highest probability given e. This is often called the maximum a posteriori estimate, or MAP estimate.\n",
    "\n",
    "#### Calculating Joint Probabilities\n",
    "\n",
    "In general, the joint probability for variable assignments $x_1,\\ldots, x_n$ is given by\n",
    "\n",
    "$$P(x_1,\\ldots, x_n) = \\prod_{i=1}^n P(x_i\\ |\\ \\text{Parents}(x_i))$$\n",
    "\n",
    "where we can simply read $P(x_i\\ |\\ \\text{Parents}(x_i))$ out of the conditional distribution attached to node `x_i`. In the context of our specific example, say we want to know the probability `B = True`, `E = False`, `A = True`, `J = True`, `M = True` (the probability that we got a call from John and Mary, the alarm has gone off, there has been a burglary, and there has been no earthquake). Using the general formula above, we get this joint probability as:\n",
    "\n",
    "$$\\begin{align*}\n",
    "P(M, J, A, not E, B) &= P(M | A) \\cdot P(J |A) \\cdot P(A | not E, B) \\cdot P(not E) \\cdot P(B) \\\\\n",
    "                     &= 0.7 \\cdot 0.9 \\cdot 0.94 \\cdot 0.998 \\cdot 0.001 \\\\\n",
    "                     &= 0.00059\n",
    "\\end{align*}$$\n",
    "\n",
    "This functionality is implemented in the provided `BayesNet` class, which we can query directly as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(M, J, A, not E, B) = 0.0005910156\n"
     ]
    }
   ],
   "source": [
    "joint_prob = alarm_net.calc_joint([(mary_node, True), \n",
    "                                   (john_node, True),\n",
    "                                   (alarm_node, True),\n",
    "                                   (earthquake_node, False),\n",
    "                                   (burglary_node, True)])\n",
    "print('P(M, J, A, not E, B) = {}'.format(joint_prob))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating Posterior Probabilities\n",
    "\n",
    "With this ability to query the joint distribution, we can answer a broad range of other questions. For instance, if we want to know the posterior probability of some event $X$ after observing some evidence $e$, we can calculate\n",
    "\n",
    "$$P(X|e) = \\alpha P(X, e) = \\alpha \\sum_{y \\notin \\{X,e\\}} P(X, e, y)$$\n",
    "\n",
    "where $\\alpha$ is a normalization constant set so that $P(X|e) + P(not X|e) = 1$. In our example, if we want to know the probability of a burglary having occured if we receive a call from John and Mary, we can calculate\n",
    "\n",
    "$$\\begin{align*}\n",
    "P(B|J, M) = \\alpha P(B, J, M) &= \\alpha \\sum_{a \\in \\{A, not A\\}}\\sum_{e \\in \\{E, notE\\}} P(B, J, M, a, b, e)\n",
    "\\end{align*}$$\n",
    "\n",
    "where $P(B, J, M, a, b, e)$ is a joint probability calculated as shown above. Substituting the equation for joint probabilities into this expression, we get \n",
    "\n",
    "$$\\begin{align*}\n",
    "P(B | J, M) = \\alpha P(B, J, M) &= \\alpha \\sum_{a \\in \\{A, not A\\}} \\sum_{e \\in \\{E, notE\\}} P(B, J, M, a, B, e) \\\\\n",
    "                              &= \\alpha \\sum_{a \\in \\{A, not A\\}}\\sum_{e \\in \\{E, notE\\}} P(M | a) \\cdot P(J | a) \\cdot P(a | e, B) \\cdot P(e) \\cdot P(B)\n",
    "\\end{align*}$$\n",
    "\n",
    "This is a convenient feature, but without further optimization it is pretty inefficient: we need to multiply $n$ values for each of $2^n$ combinations of variable assignments, so this method has time complexity $O(n2^n)$, which again puts us on the losing side of an exponential term. We've implemented this brute-force algorithm (as described in AIMA 14.4.1) in the cell below. You can see how it performs on the simple network we've defined above; later, we'll benchmark the performance of this algorithm compared to more efficient algorithms when run on larger networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def brute_force_query(X, e, net):\n",
    "    \"\"\"Returns the NON-NORMALIZED posterior probability P(X=x | e) using the Bayesian network `net`,\n",
    "    using a brute force enumeration method with time complexity O(n2^n), where\n",
    "    n is the number of variables in the network.\n",
    "    \n",
    "    Inputs:\n",
    "        X: a list of tuples (name, value) specifying assignments to query variables\n",
    "        e: a list of tuples (name, value) specifying assignments to evidence variables\n",
    "            (we assume that e and X are disjoint)\n",
    "        net: the bayesian network to calculate over\n",
    "    Outputs:\n",
    "        P: the probability that X=X given e\n",
    "    \"\"\"\n",
    "    # initialize probability\n",
    "    P = 0\n",
    "    \n",
    "    # convert all variable names to nodes in the net\n",
    "    X_nodes = [net.get_node(var[0]) for var in X]\n",
    "    e_nodes = [net.get_node(var[0]) for var in e]\n",
    "    \n",
    "    # extract all assignments to X and e variables\n",
    "    X_assignments = [var[1] for var in X]\n",
    "    e_assignments = [var[1] for var in e]\n",
    "    # a list of (node, assignment) tuples for use in joint queries\n",
    "    X_assignments = zip(X_nodes, X_assignments)\n",
    "    X_assignments = [tuple(x) for x in X_assignments]\n",
    "    e_assignments = zip(e_nodes, e_assignments)\n",
    "    e_assignments = [tuple(x) for x in e_assignments]\n",
    "    \n",
    "    # get the list of variables not in X or e\n",
    "    Y_nodes = [node for node in net.nodes if node not in X_nodes and node not in e_nodes]\n",
    "    \n",
    "    # now we sum the joint probability for all possible combinations of assignments to y\n",
    "    y_assignments = list(itertools.product(*[list(var.domain) for var in Y_nodes]))\n",
    "    \n",
    "    for assignment in y_assignments:\n",
    "        P += net.calc_joint(X_assignments + e_assignments + list(zip(Y_nodes, assignment)))\n",
    "    \n",
    "    return P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this method, we can determine the probability of a burglary having occured after having received a call from both John and Mary. Don't forget to normalize!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(B | j, m) = [0.284, 0.716]\n"
     ]
    }
   ],
   "source": [
    "P_burglary = brute_force_query([('Burglary', True)], [('John', True), ('Mary', True)], alarm_net)\n",
    "P_not_burglary = brute_force_query([('Burglary', False)], [('John', True), ('Mary', True)], alarm_net)\n",
    "\n",
    "# normalize!\n",
    "alpha = 1/(P_burglary + P_not_burglary)\n",
    "P_burglary *= alpha\n",
    "P_not_burglary *= alpha\n",
    "\n",
    "print(\"P(B | j, m) = [{}, {}]\".format(round(P_burglary, 3), round(P_not_burglary, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This works well, but $O(n2^n)$ is horribly slow! It works OK for the simple alarm network, which only has 5 variables ($5*2^5 = 160$), but if we add even just 5 more variables, we would require two orders of magniture more operations (since $10*2^10 = 10240$).\n",
    "\n",
    "To get some efficiency improvements, we can exploit the structure inherent in this network to compute probabilities more easily. Recall that although we could theoretically order the nodes any way we want, we've chosen to structure the alarm network so that it flows from causes to effects. Because of this, we can order the variables so that causes precede effects, which means the equation for the conditional probability from above becomes:\n",
    "\n",
    "$$\\begin{align*}\n",
    "P(B | J, M) &= \\alpha \\sum_{a \\in \\{A, not A\\}}\\sum_{e \\in \\{E, notE\\}} P(B) \\cdot P(e) \\cdot P(a | e, B) \\cdot P(M | a) \\cdot P(J | a)\n",
    "\\end{align*}$$\n",
    "\n",
    "We can reorder the summations and pull some of these terms outside of the sums, since $P(B)$ does not depend on either $a$ nor $e$, and $P(e)$ does not depend on the value of $a$. Thus, we get\n",
    "\n",
    "$$\\begin{align*}\n",
    "P(B | J, M) &= \\alpha P(B) \\cdot \\sum_{e \\in \\{E, notE\\}} P(e) \\cdot \\sum_{a \\in \\{A, not A\\}} P(a | e, B) \\cdot P(M | a) \\cdot P(J | a)\n",
    "\\end{align*}$$\n",
    "\n",
    "This can noticeably reduce the number of expressions we need to evaluate, since we can enumerate the terms in this equation in a depth-first manner, as shown in the algorithm below (and described by Russel & Norvig in Figure 14.9 of AIMA). This algorithm boosts our time complexity from $O(n2^n)$ to $O(2^n)$, which is definitely noticeable but still unfortunately exponential. This is better, but still much slower than what we would like for inference on larger networks. In the next section, we'll discuss an algorithms for efficiently doing inference in polynomial time.\n",
    "\n",
    "The depth-first enumeration query uses the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CITE: Russel & Norvig AIMA Ch 14, Section 4, Fig. 14.9\n",
    "def simple_query(X, e, net):\n",
    "    \"\"\"Returns the NON-NORMALIZED posterior probability P(X=X | e) using the Bayesian network `net`,\n",
    "    using an improved enumeration method with time complexity O(2^n), where n is\n",
    "    the number of variables in the network, using analogue to depth-first search.\n",
    "    \n",
    "    Inputs:\n",
    "        X: a list of tuples (name, value) specifying assignments to query variables\n",
    "        e: a list of tuples (name, value) specifying assignments to evidence variables\n",
    "            (we assume that e and X are disjoint)\n",
    "        net: the bayesian net to calculate over\n",
    "    Outputs:\n",
    "        P: the probability that X=X given e\n",
    "    \"\"\"\n",
    "    # augment e with known values of X\n",
    "    augmented_e = X+e\n",
    "    augmented_e = dict(augmented_e)\n",
    "    # get names of all variables, in topographical order (from fewest parents to most parents)\n",
    "    variables = [node.name for node in net.get_topological_ordering()]\n",
    "    # recursively compute conditional probability\n",
    "    return enumerate_all(variables, augmented_e, net)\n",
    "\n",
    "def enumerate_all(variables, e, net):\n",
    "    \"\"\"Computes one step in the depth-first iteration of the Bayesian network\n",
    "    \n",
    "    Inputs:\n",
    "        variables: a list of the names of variables to be enumerated, ordered from parents to children\n",
    "        e: a dict with keys name and values specifying assignments to variables of that name\n",
    "        net: the bayesian net to calculate over\n",
    "    Outputs:\n",
    "        P: the marginal probability P(e=e)\n",
    "    \"\"\"\n",
    "    # base case\n",
    "    if len(variables) == 0:\n",
    "        return 1.0\n",
    "    \n",
    "    # get first and rest of variables to enumerate\n",
    "    y_first = variables[0]\n",
    "    y_first_node = net.get_node(y_first)\n",
    "    y_rest = variables[1:]\n",
    "    \n",
    "    # note that since we requrie variables to be ordered so that parents appear before children,\n",
    "    # we know that all parents of y_first will already be in e (if y_first has parents)\n",
    "    parent_assignments = []\n",
    "    for parent in y_first_node.parents:\n",
    "        assert (parent.name in e), \"Variable names must be ordered so that parents appear before all their children\"\n",
    "        parent_assignments.append((parent, e[parent.name]))\n",
    "    \n",
    "    if y_first in e:\n",
    "        # then y_first has an assignment specified in e\n",
    "        y_first_val = e[y_first]\n",
    "        return y_first_node.get_prob_value(y_first_val, parent_assignments) * enumerate_all(y_rest, e, net)\n",
    "    else:\n",
    "        # sum over all possible assignments of y_first\n",
    "        sum = 0\n",
    "        # we'll augment e with each possible assignment of y, so we need a copy\n",
    "        e_aug = e.copy()\n",
    "        for y_first_val in y_first_node.domain:\n",
    "            e_aug[y_first] = y_first_val\n",
    "            sum += y_first_node.get_prob_value(y_first_val, parent_assignments) * enumerate_all(y_rest, e_aug, net)\n",
    "        return sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like we could using the brute-force method, we can use this method to determine the probability of a burglary having occured after having received a call from both John and Mary. Again, don't forget to normalize!\n",
    "\n",
    "(Hopefully the answer we get in this cell matches the probability we calculated above using the brute-force method)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(B | j, m) = [0.284, 0.716]\n"
     ]
    }
   ],
   "source": [
    "P_burglary = simple_query([('Burglary', True)], [('John', True), ('Mary', True)], alarm_net)\n",
    "P_not_burglary = simple_query([('Burglary', False)], [('John', True), ('Mary', True)], alarm_net)\n",
    "\n",
    "# normalize!\n",
    "alpha = 1/(P_burglary + P_not_burglary)\n",
    "P_burglary *= alpha\n",
    "P_not_burglary *= alpha\n",
    "\n",
    "print(\"P(B | j, m) = [{}, {}]\".format(round(P_burglary, 3), round(P_not_burglary, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compare the performance of these two different methods, we can use the slightly more complicated cyberattack example. Run the below cell to see whether we observe a speedup from the simple query method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# For a more scientific benchmark, we can average the performance over several function calls.\n",
    "N = 1000\n",
    "simple_runtimes = []\n",
    "brute_runtimes = []\n",
    "\n",
    "for i in range(N):\n",
    "    ## Brute Force\n",
    "    # time\n",
    "    brute_force_start = time.time()\n",
    "\n",
    "    # given that workstation 1 is up and workstation 2 is down,\n",
    "    # what is the probability that the 1st attack vector is being exploited\n",
    "    P_a1 = brute_force_query([('Attack1', 'Attacked')], [('Workstation1', 'Up'), ('Workstation2', 'Down')], star_net)\n",
    "    P_not_a1 = brute_force_query([('Attack1', 'Safe')], [('Workstation1', 'Up'), ('Workstation2', 'Down')], star_net)\n",
    "\n",
    "    # normalize!\n",
    "    alpha = 1/(P_burglary + P_not_burglary)\n",
    "    P_burglary *= alpha\n",
    "    P_not_burglary *= alpha\n",
    "\n",
    "    # end timer\n",
    "    brute_force_stop = time.time()\n",
    "\n",
    "    ## Simple query\n",
    "    # time\n",
    "    simple_start = time.time()\n",
    "\n",
    "    # given that workstation 1 is up and workstation 2 is down,\n",
    "    # what is the probability that the 1st attack vector is being exploited\n",
    "    P_a1 = simple_query([('Attack1', 'Attacked')], [('Workstation1', 'Up'), ('Workstation2', 'Down')], star_net)\n",
    "    P_not_a1 = simple_query([('Attack1', 'Safe')], [('Workstation1', 'Up'), ('Workstation2', 'Down')], star_net)\n",
    "\n",
    "    # normalize!\n",
    "    alpha = 1/(P_burglary + P_not_burglary)\n",
    "    P_burglary *= alpha\n",
    "    P_not_burglary *= alpha\n",
    "\n",
    "    # end timer\n",
    "    simple_stop = time.time()\n",
    "    \n",
    "    simple_runtimes.append(simple_stop - simple_start)\n",
    "    brute_runtimes.append(brute_force_stop - brute_force_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    | Brute-Force Query | Simple Query\n",
      "------------------------------------------------------\n",
      "  Avg. Runtime (ms) |          1.666084 |    0.509396 \n",
      "Std. deviation (ms) |          0.346077 |    0.179902 \n"
     ]
    }
   ],
   "source": [
    "simple_mean = 1000*np.mean(simple_runtimes)\n",
    "brute_mean = 1000*np.mean(brute_runtimes)\n",
    "simple_std = 1000*np.std(simple_runtimes)\n",
    "brute_std = 1000*np.std(brute_runtimes)\n",
    "\n",
    "print(\"                    | Brute-Force Query | Simple Query\")\n",
    "print(\"------------------------------------------------------\")\n",
    "print(\"  Avg. Runtime (ms) | {:17.6f} | {:11.6f} \".format(brute_mean, simple_mean))\n",
    "print(\"Std. deviation (ms) | {:17.6f} | {:11.6f} \".format(brute_std, simple_std))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's pretty good! We shave $67\\%$ off of the query time! But, as we'll see in the next section, we can still do better by using more sophisticated algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bucket Elimination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of implementing brute force and jumping straight to find the assignment that maximizes probability, we can eliminate some variables. The complexity of each summation is exponential in the number of variables being summed over. The brute force method is particularly slow because of the number of variables being summed over. We can instead create smaller sums by moving sums within products in the expression of the joint distribution. We leverage the fact that for any function $f$ and $g$ and random variables $X$ and $Y$: \n",
    "$$\\sum_{X} \\prod_{X,Y} g(Y)f(X,Y) = \\prod_{X,Y} g(Y) \\sum_X f(X,Y)$$\n",
    "Using our previous example, we have the joint distribution: \n",
    "$$P(M,J) = \\sum_{A,B,E} P(A|B,E)P(E)P(B)P(M|A)P(J|A)$$ \n",
    "We can now create smaller sums by moving sums within products, or eliminate variables by marginalization: \n",
    "$$P(M,J) = \\sum_{A} P(J|A)P(M|A) \\sum_{B} P(B) \\sum_{E} P(E) P(A|B,E) $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When maximizing a distribution, the number of elements maximized over is exponential in the number of variables we are maximizing. As in the case of moving sums within products, we can also reduce the number of elements maximized over by moving maximizations within products. We use the fact that for any function $f$ and $g$ and random variable $X$ with domain $D_X$ and random variable $Y$: \n",
    "$$ \\max_{X \\in D_X} \\prod_{X,Y} f(X,Y) g(Y) = \\prod_{X,Y} g(Y) \\max_{X \\in D_X} f(X,Y)$$  \n",
    "\n",
    "Returning to our example: \n",
    "$$\\max_{A,B,E} P(M,J,A,B,E) = \\max_{A,T,N}\\sum_{A,B,E} P(A|B,E)P(E)P(B)P(M|A)P(J|A)$$\n",
    "$$\\max_{A,B,E} P(M,J,A,B,E) = \\max_{A} \\sum_{A} P(J|A)P(M|A) \\max_{B} \\sum_{B} P(B) \\max_{E} \\sum_{E} P(E) P(A|B,E) $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understanding that eliminating variables simplifies the calculation of joint distributions and determining the Maximum A Posteriori Hypothesis, we study and implement a systematic approach to eliminate variables. This procedure is called Bucket Elimination. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use Bucket Elimination to marginalize variables of a joint distribution. In bucket $x_i$ we place all factors containing $x_i$. For the bucket of variable $N$ we may have: \n",
    "\n",
    "$$ Bucket(N) = {P(M|A,N),P(N)}$$ \n",
    "\n",
    "To eliminate the variable $x_i$, we multiply the factors and sum out $x_i$. For our example Bucket(N), we define \n",
    "\n",
    "$${\\lambda_{N}}(M,A) = \\sum_{N} P(M|A,N)P(N). $$ \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-7-1bae83b15f94>, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-7-1bae83b15f94>\"\u001b[0;36m, line \u001b[0;32m6\u001b[0m\n\u001b[0;31m    bucket = dict()\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "def BucketElimination(P,X,A,e):\n",
    "#Input: A belief network{P1, ..., Pn}, ordered random variables X, hypothesis A, and observation E = e, where A, E  X.\n",
    "#Output: MAP, given e.\n",
    "\n",
    "# 1. Initialize: distribute factors Pi over bucket1, ... , bucketn, where bucketi contains all factors whose highest variable is Xi.\n",
    "    bucket = dict() \n",
    "    for x in X: \n",
    "        bucket[x]=list()\n",
    "        for p in P: \n",
    "            if p.name == x or x in p.parents: \n",
    "                bucket[x].append(p)\n",
    "                P.remove(p)\n",
    "            \n",
    "# 2. Process buckets \n",
    "    Xr = X.reverse() \n",
    "    for p in Xr, \n",
    "        for factor in bucket[p]:\n",
    "            if factor.state == p and p in e:\n",
    "                \n",
    "            \n",
    "            #assign Xp = xp to each i and add to the appropriate bucket.\n",
    "        else:\n",
    "        #multiply and sum or maximize and add to the appropriate bucket \n",
    "\n",
    "# 3. Forward: Assign values to A.\n",
    "#Variable ordering is restricted: max-buckets precede (be processed after) summation buckets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of Bucket Elimination Performance "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Illustration of Bucket Elimination in Star Wars Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approximate Inference:\n",
    "\n",
    "So far in this notebook, we've stuck with relatively small nets and simple distributions. That means that doing exact inference - calculating analytically exactly what some distribution will look like - is possible. For a lot of problems that we care about, though, exact inference isn't possible. There are lots of reasons this might happen: some distribution is wonky and therefore can't be reasoned about analytically, a net is so complex that doing all the math to marginalize out variables seems impossible, etc.\n",
    "\n",
    "But we don't have to give up. In the following examples, we'll implement two sorts of approximate inference techniques: rejection sampling and Gibbs sampling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rejection Sampling:\n",
    "One of the simplest approximate inference algorithms is called rejection sampling. The basic idea is to generate samples from the joint distribution (so, sample all the variables), and then reject any samples that don't match the evidence. By keeping track of the samples that do match the evidence, you can slowly generate examples of what the net looks like when fitting the evidence. In other words, you get a distribution over the joint, conditioned on the evidence, which is exactly what you want.\n",
    "\n",
    "There is a key drawback to rejection sampling, though: if the evidence you are conditioning on is unlikely, you'll have to reject lots of samples before you have a reasonable number of samples you can keep.\n",
    "\n",
    "In the next cell, we implement rejection sampling and produce both the inferred distribution and metrics that track how many samples we had to reject. You'll see that as we condition on rarer events, the percentage of samples we reject increases substantially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_state_tracker(X, net):\n",
    "    var_to_val_to_count = {}\n",
    "    for x in X:\n",
    "        var_to_val_to_count[x] = {}\n",
    "        for val in net.get_node(x).domain:\n",
    "            var_to_val_to_count[x][val] = 0\n",
    "    return var_to_val_to_count\n",
    "\n",
    "# Helper method that establishes if a node being set to a value matches the evidence given.\n",
    "def matches_evidence(node, value, evidence):\n",
    "    for evidence_name, evidence_value in evidence:\n",
    "        if node.name == evidence_name:\n",
    "            if value == evidence_value:\n",
    "                return True\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def compute_dist_from_history(var_to_val_to_count):\n",
    "    for x in var_to_val_to_count.keys():\n",
    "        relevant_counts = var_to_val_to_count.get(x)\n",
    "        total_count = sum([count for count in relevant_counts.values()])\n",
    "        normalized_distribution = {}\n",
    "        for value, count in relevant_counts.items():\n",
    "            normalized_distribution[value] = count / total_count\n",
    "        print(\"Distribution for \", x, \":\", normalized_distribution)\n",
    "    \n",
    "# Implementation of rejection sampling.\n",
    "# X: list of strings of the names of nodes we want to get the distribution of\n",
    "# e: list of tuples of (variable name, assignment) that we use as evidence to condition on\n",
    "# net: the BayesNet object describing the probabilistic relationships between the nodes.\n",
    "# num_samples: integer number of samples to generate. (Note: this is not the number of samples to keep, just to generate.)\n",
    "def rejection_sampling(X, e, net, num_samples=10000):\n",
    "    # For each of the variables in X, store a count of how often each value in the domain appears.\n",
    "    # Intialization to zero counts everywhere.\n",
    "    var_to_val_to_count = initialize_state_tracker(X, net)\n",
    "    \n",
    "    # Now generate samples in the net, rejecting a sample if it doesn't match the evidence.\n",
    "    num_samples_rejected = 0\n",
    "    for sample_idx in range(num_samples):\n",
    "        # Get a topological ordering to start sampling.\n",
    "        ordered_nodes = net.get_topological_ordering()\n",
    "        assignments = {}\n",
    "        reject_sample = False\n",
    "        for node in ordered_nodes:\n",
    "            if node.marginal_distribution:\n",
    "                sample = node.draw_sample()\n",
    "                assignments[node] = sample\n",
    "            else:\n",
    "                parent_val_assignments = [(parent, assignments.get(parent)) for parent in node.parents]\n",
    "                sample = node.draw_sample(parent_vals=parent_val_assignments)\n",
    "                assignments[node] = sample\n",
    "            # Reject if the node that was sampled contradicts the evidence\n",
    "            if not matches_evidence(node, sample, e):\n",
    "                reject_sample = True\n",
    "                break  # No point in continuing to sample further nodes if one already doesn't match e.\n",
    "        if reject_sample:\n",
    "            num_samples_rejected += 1\n",
    "            continue\n",
    "        # Matched the evidence, so update the counts of valid variable assignments\n",
    "        for assigned_node, assigned_val in assignments.items():\n",
    "            if assigned_node.name in var_to_val_to_count.keys():\n",
    "                var_to_val_to_count[assigned_node.name][assigned_val] += 1\n",
    "    # We have the counts that we can use to compute probabilities, so do the final synthesis.\n",
    "    compute_dist_from_history(var_to_val_to_count)\n",
    "    # And print out metrics about how many samples were rejected\n",
    "    print(\"Percentage of samples rejected\", num_samples_rejected / num_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution for  Attack1 : {'Attacked': 0.16601752677702045, 'Safe': 0.8339824732229796}\n",
      "Percentage of samples rejected 0.3838\n",
      "Distribution for  Attack1 : {'Attacked': 0.17741347905282331, 'Safe': 0.8225865209471767}\n",
      "Distribution for  Subsystem2 : {'Offline': 0.22586520947176686, 'Online': 0.7741347905282332}\n",
      "Percentage of samples rejected 0.7255\n"
     ]
    }
   ],
   "source": [
    "# Test the rejection sampling code.\n",
    "# First, a really simple example.\n",
    "rejection_sampling(['Attack1'], [('Workstation1', 'Up')], star_net)\n",
    "# Now, a harder one, with evidence that is less likely. The percentage of rejections increases substantially.\n",
    "rejection_sampling(['Attack1', 'Subsystem2'], [('Workstation4', 'Down'), ('Subsystem1', 'Online')], star_net)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gibbs Sampling\n",
    "Now that we have a handle on rejection sampling, let's see if we can use some of the same ideas (sampling instead of exact calculations) without paying the cost of a huge number of samples that we discard.\n",
    "\n",
    "In Gibbs sampling, the basic idea is to form a Markov Chain (MC) over the joint states. In one timestep, the MC will have a full assignment. In the next, exactly one of the variables in the net will be randomly sampled, conditioned on its Markov blanket. That means that either zero or one variables changes at each timestep.\n",
    "\n",
    "It turns out (and AIMA explains the theory in far more depth), that if you run an MC like this for a long time, assuming some nice properties of the Bayes Net, the distribution of joint states visited in the MC exactly matches the joint distribution of the net. Furthermore, conditioning on variables being assigned to specific values becomes extremely easy: when transitioning to the next state in the MC, never change the evidence variables from their assigned values. That means that every single state the MC visits will match the evidence, neatly fixing the issue with rejection sampling's inefficiency.\n",
    "\n",
    "Unfortunately, Gibbs sampling isn't just a pure win. How one intializes all the variables to start of the MC can matter, at least in the short term. Imagine if the net were initialized to some extremely unlikely configuration and the MC were only run for a short time. The empirical distribution would likely not reflect the true distribution. The good news is that, over time, the effects of the initialization should wear off. That's why many researchers use what's called a \"burn in\" period - some number of samples where you run the MC after initialization but without recording any of the data. How long is long enough for burn in? That's an open research question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_assignments(net):\n",
    "    assignments = {}\n",
    "    ordered_nodes = net.get_topological_ordering()\n",
    "    for node in ordered_nodes:\n",
    "        if node.marginal_distribution:\n",
    "            sample = node.draw_sample()\n",
    "            assignments[node] = sample\n",
    "        else:\n",
    "            parent_val_assignments = [(parent, assignments.get(parent)) for parent in node.parents]\n",
    "            sample = node.draw_sample(parent_vals=parent_val_assignments)\n",
    "            assignments[node] = sample\n",
    "    return assignments\n",
    "\n",
    "def gibbs_sampling(X, e, net, burn_in_period=100, eval_period=5000):\n",
    "    # For each of the variables in X, store a count of how often each value in the domain appears.\n",
    "    var_to_val_to_count = initialize_state_tracker(X, net)\n",
    "    \n",
    "    evidence_names = [evidence[0] for evidence in e]\n",
    "    \n",
    "    # Intialize the net with random assignments.\n",
    "    assignments = initialize_assignments(net)\n",
    "    \n",
    "    # Run the MC.\n",
    "    all_nodes = net.nodes\n",
    "    for trial in range(burn_in_period + eval_period):\n",
    "        # Choose a random node, as long as it's not evidence.\n",
    "        node_to_swap = np.random.choice(all_nodes)\n",
    "        while node_to_swap.name in evidence_names:\n",
    "            node_to_swap = np.random.choice(all_nodes)\n",
    "        # Generate the distribution over next possible values of the node, conditioned on the Markov blanket of the node.\n",
    "        parent_assignments = [(parent, assignments.get(parent)) for parent in node_to_swap.parents]\n",
    "        children = net.get_children(node_to_swap)\n",
    "        \n",
    "        # For each possible next value the node could take, find the likelihood, conditioned on the\n",
    "        # Markov blanket.\n",
    "        next_distribution = {}\n",
    "        for next_val in node_to_swap.domain:\n",
    "            prob_given_parents = node_to_swap.get_prob_value(next_val, parent_assignments)\n",
    "            # Calculate the probability of the children's values given the next assignment\n",
    "            prob_of_children = 1.0\n",
    "            for child in children:\n",
    "                childs_parent_assignments = []\n",
    "                # For the child's parents, take the existing values except for the value for the current node,\n",
    "                # which must be replaced with next_val.\n",
    "                for parent in child.parents:\n",
    "                    if parent == node_to_swap:\n",
    "                        childs_parent_assignments.append((node_to_swap, next_val))\n",
    "                        continue\n",
    "                    childs_parent_assignments.append((parent, assignments.get(parent)))\n",
    "                prob_of_child = child.get_prob_value(assignments.get(child), childs_parent_assignments)\n",
    "                prob_of_children = prob_of_children * prob_of_child\n",
    "            total_prob = prob_given_parents * prob_of_children\n",
    "            next_distribution[next_val] = total_prob\n",
    "        # Normalize the distribution.\n",
    "        normalizing_factor = sum(next_distribution.values())\n",
    "        for entry, val in next_distribution.items():\n",
    "            next_distribution[entry] = val / normalizing_factor\n",
    "        # Sample from the distribution to make the assignment.\n",
    "        values = [entry[0] for entry in sorted(next_distribution.items())]\n",
    "        probabilities = [entry[1] for entry in sorted(next_distribution.items())]\n",
    "        sampled = np.random.choice(values, p=probabilities)\n",
    "        assignments[node_to_swap] = sampled\n",
    "        # If after burn-in, start saving data\n",
    "        if trial <= burn_in_period:\n",
    "            continue\n",
    "        # Save the data.\n",
    "        for assigned_node, assigned_val in assignments.items():\n",
    "            if assigned_node.name in var_to_val_to_count.keys():\n",
    "                var_to_val_to_count[assigned_node.name][assigned_val] += 1\n",
    "\n",
    "    compute_dist_from_history(var_to_val_to_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test out Gibbs with the same examples as for rejection sampling\n",
    "gibbs_sampling(['Attack1'], [('Workstation1', 'Up')], star_net)\n",
    "print()\n",
    "gibbs_sampling(['Attack1', 'Subsystem2'], [('Workstation4', 'Down'), ('Subsystem1', 'Online')], star_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
