{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Networks for Estimating Attack Vector\n",
    "\n",
    "## Intro & motivating example\n",
    "\n",
    "Humans are notoriously bad at reasoning about probabilities: more often than not, we rely on cognitive shortcuts (heuristics and biases) instead of actually calculating the liklihood of events. This can be a good thing; just think how paralyzed we would be if we stopped to calculate the probability of rain every time we heard raindrops on our window, rather than just grabbing an umbrella! Even computers can start to run into tractability problems when working with probability distributions involving even a few dozen variables. In this tutorial, we will introduce a tool, called Bayesian networks, for automatically and efficiently reasoning about probabilities. These networks will allow us to ask questions like \"given this new evidence, what is the probability of some hidden variable being true\" or \"what is the most likely explanation for some set of observations?\". These questions might remind you of our lectures on Hidden Markov Models (HMMs); it turns out that Bayesian networks can be used to represent Markov processes, but they are much more expressive and can be used to model much more complicated scenarios.\n",
    "\n",
    "To make this discussion more concrete, it's helpful to have a motivating example. You are a security engineer in the employ of the Generic Galactic Empire aboard a controversial new ~~moon~~ space station. You are aware of several security vulnerabilities in the space station's software subsystems, but you have no way to detect whether those vulnerabilities are being exploited except by running diagnostics on various workstations throughout the station. Since you'd like to be able to detect attacks based on the probability that a vulnerability is being exploited, you'd like an easy way to relate your observations of workstations to the probability of a cyberattack taking place. After a brief review of probability fundamentals, we'll discuss how Bayesian networks can be used to model this situation and make inferences automatically.\n",
    "\n",
    "### References\n",
    "The discussion in this tutorial draws heavily from the following sources:\n",
    "- Russell & Norvig, AIMA, Chapter 13 & 14\n",
    "- [R. Dechter, \"Bucket Elimination: A Unifying Framework for Probabilistic Inference\"](https://webdocs.cs.ualberta.ca/~rgreiner/C-366/RG-2002-SLIDES/BucketElim.pdf) \n",
    "\n",
    "Chapter 13 of AIMA provides an overview of probability fundamentals, while chapter 14 discusses Bayesian networks in more depth. Dechter's article describes efficient algorithms for making inference based on Bayesian networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability Review\n",
    "\n",
    "This section will provide a quick review of relevant probability concepts. For a more thorough review, see Russel & Norvig AIMA, Chapter 13.\n",
    "\n",
    "Given a set of $n$ discrete random variables, we can fully represent the probabilities and relationships between all $n$ variables using a table called the **joint distribution**. This table has a dimension for every variable and a row for each possible assignment to that variable; a joint distribution for 2 binary variables, $A$ and $B$, would have a row for each value of $A$ and a column for each value of $B$. Each entry in the joint distribution contains the probability that the corresponding assignment to each of the variables will be observed; in the $2\\times2$ case, the $(0, 0)$ entry would contain the probability that $A=0$ and $B=0$.\n",
    "\n",
    "Formally, the joint distribution allows us to lookup probabilities $P(A\\land B \\land C \\land ...)$ just be reading the value out of the joint distribution table. They also allow us to find the so-called *marginal probability* of some variable $P(A)$, which is the probability that $A$ takes on some value regardless of the value of all other variables. In the binary case, $P(A) = P(A\\land B) + P(A \\land \\text{not }B}$; in the general case, we can calculate the marginal probability by summing over all possible values of the variables we don't care about, like so: $$P(A) = \\sum_{\\text{all possible assignments to }B,C,\\ldots} P(A \\land B \\land C \\land \\ldots$$\n",
    "\n",
    "Often, we are interested in the case when we have observed some variable assignments (which we call the evidence variables $e$), and we are interested in the probability of some other variables ($X$) given those observations. This probability $P(X | e)$ is called the **conditional** or **posterior probability**, and it can be calculated using the product rule, $P(A\\land e) = P(X|e) P(e)$, which we can rearrange to find \n",
    "\n",
    "$$P(X | e) = P(X\\land e)/P(e) = \\alpha P(X \\land e)$$\n",
    "\n",
    "where $\\alpha$ is a normalization factor calculated so that $P(X | e) + P(\\text{not }X | e) = 1$. Note that $P(X \\land e)$ is simply a marginal probability, so we can compute it from the joint distribution as $$P(X\\land e) = \\sum_{\\text{all possible assignments to }y\\text{ not in}x, e}P(X \\land e \\land y)$$\n",
    "\n",
    "    - Conditional/posterior probabilities, product rule P(A ^ B) = P(A | B)P(B), and conditioning (marginalization but with conditional probs).\n",
    "    - Derive P(X|e) = P(X^e)/P(e) = aP(X^e) = a \\sum_y P(X, e, y) (normalization to get a so that P(X|e)+P(!X|e)=1\n",
    "    - Remark: this would allow us to answer whatever queries we want to ask just by looking in the joint distribution table. BUT that table has size O(2^n) and requires O(2^n) to process by summing over it.\n",
    "- Reasoning using conditional distributions (13.4)\n",
    "    - Lots of redundant information in full joint distribution (like how your toothache affects the weather, ignoring causality)\n",
    "    - Independence! Conditional independence (based on causality)!\n",
    "    - Allows factoring the joint distribution into multiple smaller conditional distributions, saving on size. 2^n+2^m << 2^{n+m}    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Networks\n",
    "\n",
    "Exploiting conditional independence can allow us to reason much more efficiently about joint probabilities, but it would be really nice if we could automate the inference process by encoding conditional probabilities in a convenient data structure. Since dependence relates each variable to a set of \"parent\" variables, a natural structure is a directed graph, where\n",
    "- Each node corresponds to an uncertain variable (in this tutorial, we'll restrict ourselves to discrete random variables),\n",
    "- If variable $B$ is dependent on $A$, then we draw a directed edge from node `A` to node `B` in the graph, and\n",
    "- Each node $X$ is labelled with its conditional probability distribution $P(X\\ |\\ \\text{Parents}(X))$, which takes the form of a table with a row for each combination of assignments to the parent variables of $X$, specifying the probabilities that $X$ takes on each of its values conditioned on those parent variable assignments.\n",
    "\n",
    "Since we don't want any variable to depend (directly or indirectly) on itself, this graph must not have any directed cycles (making it a *directed acyclic graph*. An example graph is shown in the figure below. Given this structure, we can see that it directly encodes both independence and conditional independence: sibling nodes (i.e. nodes sharing a parent) are conditionally independent, while nodes in disconnected graphs are independent.\n",
    "\n",
    "We call this data structure a Bayesian network (or sometimes a *belief network* or *causal network*). These networks can represent any full joint distribution, but depending on the degree to which causal relationships allow us to factor the joint distribution into smaller conditional distributions, Bayesian networks can encode this information much more compactly. In the next example, we'll give a simple example of a Bayesian network and show how we can make inference based on these networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A simple example\n",
    "\n",
    "Before moving on to the more complicated case of our original motivating problem, we'll use the simpler scenario from Fig. 14.2 in Russel & Norvig to illustrate the key features of Bayesian networks. In this scenario, you've just installed a new alarm in your house, and since you don't want Google spying on your home, you've chosen not to connect the alarm to the internet. Instead, you've asked your neighbors, John and Mary, to listen for the alarm and call you if they hear it going off. Complicating matters, the alarm will go off if a burglary occurs, but it is a little too sensitive and will also go off if an earthquake occurs. In addition, John will will sometimes confuse a telephone ringing for the alarm and call then as well, and because Mary likes listening to loud music, she will sometimes miss the alarm altogether.\n",
    "\n",
    "In this scenario, we essentially have five uncertain boolean variables:\n",
    "+ `B`: whether a burglary has occurred,\n",
    "+ `E`: whether an earthquake has occurred,\n",
    "+ `A`: whether the alarm has gone off,\n",
    "+ `J`: whether John calls you, and\n",
    "+ `M`: whether Mary calls you.\n",
    "\n",
    "Note that only `J` and `M` are observable. If we were to encode the full joint probability distribution, we'd need to store a table of size $O(2^n) = O(32)$. This is not too large in thi scase, but if you have even 30 variables then the joint distribution table would require $O(4$ GB$)$ of storage, and adding a 31st variable would require another 4 GB! We are clearly on the wrong side of this exponential when it comes to the joint distribution.\n",
    "\n",
    "To make the problem more tractable, we can exploit the causal relationships between these variables, allowing us to store a handful of smaller conditional distributions rather than a monolithic joint distribution. The causal structure of this problem leads to the Bayesian network shown in the image below (reprinted from Russell & Norvig):\n",
    "\n",
    "<img src=\"files/figs/alarm-problem-structure.png\">\n",
    "\n",
    "Since `J` and `M` only depend on `A`, we don't need to explicitly store the effect of an earthquake on the probability of John calling. Instead of a 32-entry joint distribution, we only need to store 20 values in 5 smaller conditional distributions. Only 10 entries are shown here because for boolean variables the probability of a false value is fixed at 1-(the probability of a true value). For scenarios with even more variables, these savings would be even more dramatic.\n",
    "\n",
    "It's important to note that the Bayesian network representation is not unique: you can order the variables in any way you want and back out the conditional probability tables from the joint distribution. In this case, we have ordered the nodes from causes to effects (causes at the top, flowing down to effects at the bottom of the network). This is a good heuristic for constructing concise Bayesian networks.\n",
    "\n",
    "We've written classes in `bayes_net.py` implementing the basic functionality of Bayesian networks for you, allowing you to represent these networks programmatically. You should go check out the source code to get an idea of what features are available, and a basic example of constructing a network for this example scenario is given below. To properly load the drawing function, **you must run this cell twice**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Have 5 nodes and 4 edges\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Example code for constructing the simple burglary example from AIMA (Fig. 14.2)\n",
    "from bayes_net import BayesNet, BayesNode\n",
    "import numpy as np\n",
    "\n",
    "# First, instantiate the two nodes that have no parents in the network.\n",
    "# The domain of these variables is just {True, False} here, but we support arbitrary discrete domains\n",
    "burglary_node = BayesNode('Burglary')\n",
    "burglary_node.set_marginal_distribution({True: 0.001, False: 0.999})\n",
    "earthquake_node = BayesNode('Earthquake')\n",
    "earthquake_node.set_marginal_distribution({True: 0.002, False: 0.998})\n",
    "\n",
    "# Now we can instantiate nodes with probabilities conditioned on their parents\n",
    "# We have to build the conditional distribution table one entry at a time, for each combination\n",
    "# of parent variables\n",
    "alarm_node = BayesNode('Alarm')\n",
    "alarm_node.add_entry([(burglary_node, True), (earthquake_node, True)], {True: 0.95, False: 0.05})\n",
    "alarm_node.add_entry([(burglary_node, True), (earthquake_node, False)], {True: 0.94, False: 0.06})\n",
    "alarm_node.add_entry([(burglary_node, False), (earthquake_node, True)], {True: 0.29, False: 0.71})\n",
    "alarm_node.add_entry([(burglary_node, False), (earthquake_node, False)], {True: 0.001, False: 0.999})\n",
    "\n",
    "john_node = BayesNode('John')\n",
    "john_node.add_entry([(alarm_node, True)], {True: 0.9, False: 0.1})\n",
    "john_node.add_entry([(alarm_node, False)], {True: 0.05, False: 0.95})\n",
    "\n",
    "mary_node = BayesNode('Mary')\n",
    "mary_node.add_entry([(alarm_node, True)], {True: 0.7, False: 0.3})\n",
    "mary_node.add_entry([(alarm_node, False)], {True: 0.01, False: 0.99})\n",
    "\n",
    "# Now we can create a BayesNet object to store all the nodes.\n",
    "alarm_net = BayesNet([burglary_node, earthquake_node, alarm_node, john_node, mary_node])\n",
    "\n",
    "# As a sanity check, we can visualize the network to make sure it matches our model above.\n",
    "# This visualization is auto-generated, so it will be a bit messy, but we can still see\n",
    "# the overall structure of the network.\n",
    "alarm_net.draw_net()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use similar code to construct a more complicated network based on our original motivating example (cyberattack estimation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Insert code for star wars network once we have data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference on Bayesian Networks\n",
    "\n",
    "Now that we've constructed a bayesian network, what can we do with it? In short, everything you can do with the full joint distribution! In the next few sections, we'll discuss algorithms for computing:\n",
    "- The joint probability distribution e.g. P(a, b, !c, d, !e)\n",
    "- The posterior probability given evidence e.g. P(X | e) = aP(X, e)\n",
    "- The maximum liklihood explanation of a set of evidence, which is the assignments to X with the highest probability given e. This is often called the maximum a posteriori estimate, or MAP estimate.\n",
    "\n",
    "#### Calculating Joint Probabilities\n",
    "\n",
    "In general, the joint probability for variable assignments $x_1,\\ldots, x_n$ is given by\n",
    "\n",
    "$$P(x_1,\\ldots, x_n) = \\prod_{i=1}^n P(x_i\\ |\\ \\text{Parents}(x_i))$$\n",
    "\n",
    "where we can simply read $P(x_i\\ |\\ \\text{Parents}(x_i)$ out of the conditional distribution attached to node `x_i`. In the context of our specific example, say we want to know the probability `B = True`, `E = False`, `A = True`, `J = True`, `M = True` (the probability that we got a call from John and Mary, the alarm has gone off, there has been a burglary, and there has been no earthquake). Using the general formula above, we get this joint probability as:\n",
    "\n",
    "$$\\begin{align*}\n",
    "P(M, J, A, not E, B) &= P(M | A) \\cdot P(J |A) \\cdot P(A | not E, B) \\cdot P(not E) \\cdot P(B) \\\\\n",
    "                     &= 0.7 \\cdot 0.9 \\cdot 0.94 \\cdot 0.998 \\cdot 0.001 \\\\\n",
    "                     &= 0.00059\n",
    "\\end{align*}$$\n",
    "\n",
    "This functionality is implemented in the provided `BayesNet` class, which we can query directly as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(M, J, A, not E, B) = 0.0005910156\n"
     ]
    }
   ],
   "source": [
    "joint_prob = alarm_net.calc_joint([(mary_node, True), \n",
    "                                   (john_node, True),\n",
    "                                   (alarm_node, True),\n",
    "                                   (earthquake_node, False),\n",
    "                                   (burglary_node, True)])\n",
    "print('P(M, J, A, not E, B) = {}'.format(joint_prob))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating Posterior Probabilities\n",
    "\n",
    "With this ability to query the joint distribution, we can answer a broad range of other questions. For instance, if we want to know the posterior probability of some event $X$ after observing some evidence $e$, we can calculate\n",
    "\n",
    "$$P(X|e) = \\alpha P(X, e) = \\alpha \\sum_{y \\notin \\{X,e\\}} P(X, e, y)$$\n",
    "\n",
    "where $\\alpha$ is a normalization constant set so that $P(X|e) + P(not X|e) = 1$. In our example, if we want to know the probability of a burglary having occured if we receive a call from John and Mary, we can calculate\n",
    "\n",
    "$$\\begin{align*}\n",
    "P(B|J, M) = \\alpha P(B, J, M) &= \\alpha \\sum_{a \\in \\{A, not A\\}}\\sum_{e \\in \\{E, notE\\}} P(B, J, M, a, b, e)\n",
    "\\end{align*}$$\n",
    "\n",
    "where $P(B, J, M, a, b, e)$ is a joint probability calculated as shown above. Substituting the equation for joint probabilities into this expression, we get \n",
    "\n",
    "$$\\begin{align*}\n",
    "P(B | J, M) = \\alpha P(B, J, M) &= \\alpha \\sum_{a \\in \\{A, not A\\}} \\sum_{e \\in \\{E, notE\\}} P(B, J, M, a, B, e) \\\\\n",
    "                              &= \\alpha \\sum_{a \\in \\{A, not A\\}}\\sum_{e \\in \\{E, notE\\}} P(M | a) \\cdot P(J | a) \\cdot P(a | e, B) \\cdot P(e) \\cdot P(B)\n",
    "\\end{align*}$$\n",
    "\n",
    "This is a convenient feature, but without further optimization it is pretty inefficient: we need to multiply $n$ values for each of $2^n$ combinations of variable assignments, so this method has time complexity $O(n2^n)$, which again puts us on the losing side of an exponential term. We've implemented this brute-force algorithm (as described in AIMA 14.4.1) in the cell below. You can see how it performs on the simple network we've defined above; later, we'll benchmark the performance of this algorithm compared to more efficient algorithms when run on larger networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def brute_force_query(X, e, net):\n",
    "    \"\"\"Returns the NON-NORMALIZED posterior probability P(X=x | e) using the Bayesian network `net`,\n",
    "    using a brute force enumeration method with time complexity O(n2^n), where\n",
    "    n is the number of variables in the network.\n",
    "    \n",
    "    Inputs:\n",
    "        X: a list of tuples (name, value) specifying assignments to query variables\n",
    "        e: a list of tuples (name, value) specifying assignments to evidence variables\n",
    "            (we assume that e and X are disjoint)\n",
    "        net: the bayesian network to calculate over\n",
    "    Outputs:\n",
    "        P: the probability that X=X given e\n",
    "    \"\"\"\n",
    "    # initialize probability\n",
    "    P = 0\n",
    "    \n",
    "    # convert all variable names to nodes in the net\n",
    "    X_nodes = [net.get_node(var[0]) for var in X]\n",
    "    e_nodes = [net.get_node(var[0]) for var in e]\n",
    "    \n",
    "    # extract all assignments to X and e variables\n",
    "    X_assignments = [var[1] for var in X]\n",
    "    e_assignments = [var[1] for var in e]\n",
    "    # a list of (node, assignment) tuples for use in joint queries\n",
    "    X_assignments = zip(X_nodes, X_assignments)\n",
    "    X_assignments = [tuple(x) for x in X_assignments]\n",
    "    e_assignments = zip(e_nodes, e_assignments)\n",
    "    e_assignments = [tuple(x) for x in e_assignments]\n",
    "    \n",
    "    # get the list of variables not in X or e\n",
    "    Y_nodes = [node for node in net.nodes if node not in X_nodes and node not in e_nodes]\n",
    "    \n",
    "    # now we sum the joint probability for all possible combinations of assignments to y\n",
    "    y_assignments = list(itertools.product(*[list(var.domain) for var in Y_nodes]))\n",
    "    \n",
    "    for assignment in y_assignments:\n",
    "        P += net.calc_joint(X_assignments + e_assignments + list(zip(Y_nodes, assignment)))\n",
    "    \n",
    "    return P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this method, we can determine the probability of a burglary having occured after having received a call from both John and Mary. Don't forget to normalize!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(B | j, m) = [0.284, 0.716]\n"
     ]
    }
   ],
   "source": [
    "P_burglary = brute_force_query([('Burglary', True)], [('John', True), ('Mary', True)], alarm_net)\n",
    "P_not_burglary = brute_force_query([('Burglary', False)], [('John', True), ('Mary', True)], alarm_net)\n",
    "\n",
    "# normalize!\n",
    "alpha = 1/(P_burglary + P_not_burglary)\n",
    "P_burglary *= alpha\n",
    "P_not_burglary *= alpha\n",
    "\n",
    "print(\"P(B | j, m) = [{}, {}]\".format(round(P_burglary, 3), round(P_not_burglary, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This works well, but $O(n2^n)$ is horribly slow! It works OK for the simple alarm network, which only has 5 variables ($5*2^5 = 160$), but if we add even just 5 more variables, we would require two orders of magniture more operations (since $10*2^10 = 10240$).\n",
    "\n",
    "To get some efficiency improvements, we can exploit the structure inherent in this network to compute probabilities more easily. Recall that although we could theoretically order the nodes any way we want, we've chosen to structure the alarm network so that it flows from causes to effects. Because of this, we can order the variables so that causes precede effects, which means the equation for the conditional probability from above becomes:\n",
    "\n",
    "$$\\begin{align*}\n",
    "P(B | J, M) &= \\alpha \\sum_{a \\in \\{A, not A\\}}\\sum_{e \\in \\{E, notE\\}} P(B) \\cdot P(e) \\cdot P(a | e, B) \\cdot P(M | a) \\cdot P(J | a)\n",
    "\\end{align*}$$\n",
    "\n",
    "We can reorder the summations and pull some of these terms outside of the sums, since $P(B)$ does not depend on either $a$ nor $e$, and $P(e)$ does not depend on the value of $a$. Thus, we get\n",
    "\n",
    "$$\\begin{align*}\n",
    "P(B | J, M) &= \\alpha P(B) \\cdot \\sum_{e \\in \\{E, notE\\}} P(e) \\cdot \\sum_{a \\in \\{A, not A\\}} P(a | e, B) \\cdot P(M | a) \\cdot P(J | a)\n",
    "\\end{align*}$$\n",
    "\n",
    "This can noticeably reduce the number of expressions we need to evaluate, since we can enumerate the terms in this equation in a depth-first manner, as shown in the algorithm below (and described by Russel & Norvig in Figure 14.9 of AIMA). This algorithm boosts our time complexity from $O(n2^n)$ to $O(2^n)$, which is definitely noticeable but still unfortunately exponential. This is better, but still much slower than what we would like for inference on larger networks. In the next section, we'll discuss an algorithms for efficiently doing inference in polynomial time.\n",
    "\n",
    "The depth-first enumeration query uses the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CITE: Russel & Norvig AIMA Ch 14, Section 4, Fig. 14.9\n",
    "def simple_query(X, e, net):\n",
    "    \"\"\"Returns the NON-NORMALIZED posterior probability P(X=X | e) using the Bayesian network `net`,\n",
    "    using an improved enumeration method with time complexity O(2^n), where n is\n",
    "    the number of variables in the network, using analogue to depth-first search.\n",
    "    \n",
    "    Inputs:\n",
    "        X: a list of tuples (name, value) specifying assignments to query variables\n",
    "        e: a list of tuples (name, value) specifying assignments to evidence variables\n",
    "            (we assume that e and X are disjoint)\n",
    "        net: the bayesian net to calculate over\n",
    "    Outputs:\n",
    "        P: the probability that X=X given e\n",
    "    \"\"\"\n",
    "    # augment e with known values of X\n",
    "    augmented_e = X+e\n",
    "    augmented_e = dict(augmented_e)\n",
    "    # get names of all variables, in topographical order (from fewest parents to most parents)\n",
    "    variables = [node.name for node in net.get_topographical_ordering()]\n",
    "    # recursively compute conditional probability\n",
    "    return enumerate_all(variables, augmented_e, net)\n",
    "\n",
    "def enumerate_all(variables, e, net):\n",
    "    \"\"\"Computes one step in the depth-first iteration of the Bayesian network\n",
    "    \n",
    "    Inputs:\n",
    "        variables: a list of the names of variables to be enumerated, ordered from parents to children\n",
    "        e: a dict with keys name and values specifying assignments to variables of that name\n",
    "        net: the bayesian net to calculate over\n",
    "    Outputs:\n",
    "        P: the marginal probability P(e=e)\n",
    "    \"\"\"\n",
    "    # base case\n",
    "    if len(variables) == 0:\n",
    "        return 1.0\n",
    "    \n",
    "    # get first and rest of variables to enumerate\n",
    "    y_first = variables[0]\n",
    "    y_first_node = net.get_node(y_first)\n",
    "    y_rest = variables[1:]\n",
    "    \n",
    "    # note that since we requrie variables to be ordered so that parents appear before children,\n",
    "    # we know that all parents of y_first will already be in e (if y_first has parents)\n",
    "    parent_assignments = []\n",
    "    for parent in y_first_node.parents:\n",
    "        assert (parent.name in e), \"Variable names must be ordered so that parents appear before all their children\"\n",
    "        parent_assignments.append((parent, e[parent.name]))\n",
    "    \n",
    "    if y_first in e:\n",
    "        # then y_first has an assignment specified in e\n",
    "        y_first_val = e[y_first]\n",
    "        return y_first_node.get_prob_value(y_first_val, parent_assignments) * enumerate_all(y_rest, e, net)\n",
    "    else:\n",
    "        # sum over all possible assignments of y_first\n",
    "        sum = 0\n",
    "        # we'll augment e with each possible assignment of y, so we need a copy\n",
    "        e_aug = e.copy()\n",
    "        for y_first_val in y_first_node.domain:\n",
    "            e_aug[y_first] = y_first_val\n",
    "            sum += y_first_node.get_prob_value(y_first_val, parent_assignments) * enumerate_all(y_rest, e_aug, net)\n",
    "        return sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like we could using the brute-force method, we can use this method to determine the probability of a burglary having occured after having received a call from both John and Mary. Again, don't forget to normalize!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(B | j, m) = [0.284, 0.716]\n"
     ]
    }
   ],
   "source": [
    "P_burglary = simple_query([('Burglary', True)], [('John', True), ('Mary', True)], alarm_net)\n",
    "P_not_burglary = simple_query([('Burglary', False)], [('John', True), ('Mary', True)], alarm_net)\n",
    "\n",
    "# normalize!\n",
    "alpha = 1/(P_burglary + P_not_burglary)\n",
    "P_burglary *= alpha\n",
    "P_not_burglary *= alpha\n",
    "\n",
    "print(\"P(B | j, m) = [{}, {}]\".format(round(P_burglary, 3), round(P_not_burglary, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bucket Elimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the main notebook to run.\n",
    "# In this notebook, we go through the \"attack-estimation\" portion of the project.\n",
    "# Some utility code, including base classes, lives in files outside this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A bunch of imports\n",
    "from bayes_net import BayesNet, BayesNode\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.024000000000000004\n",
      "Have 4 nodes and 3 edges\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rain\n",
      "Sprinklers\n",
      "Grass\n",
      "Feet\n"
     ]
    }
   ],
   "source": [
    "# A demonstration for developers of how to create a BayesNet. In this example, I define the likelihood of getting\n",
    "# wet feet as a function of wet grass, which is a function of raining and sprinkling.\n",
    "\n",
    "# First, declare a bunch of nodes.\n",
    "rain_node = BayesNode('Rain')\n",
    "rain_node.set_marginal_distribution({True: 0.2, False: 0.8})\n",
    "sprinklers_node = BayesNode('Sprinklers')\n",
    "sprinklers_node.set_marginal_distribution({'on': 0.6, 'off': 0.4})\n",
    "grass_node = BayesNode('Grass')\n",
    "grass_node.add_entry([(rain_node, True), (sprinklers_node, 'on')], {'wet': 0.95, 'dry': 0.05})\n",
    "grass_node.add_entry([(rain_node, True), (sprinklers_node, 'off')], {'wet': 0.6, 'dry': 0.4})\n",
    "grass_node.add_entry([(rain_node, False), (sprinklers_node, 'on')], {'wet': 0.45, 'dry': 0.55})\n",
    "grass_node.add_entry([(rain_node, False), (sprinklers_node, 'off')], {'wet': 0.1, 'dry': 0.90})\n",
    "feet_node = BayesNode('Feet')\n",
    "feet_node.add_entry([(grass_node, 'wet')], {'dry': 0.1, 'damp': 0.5, 'drenched': 0.4})\n",
    "feet_node.add_entry([(grass_node, 'dry')], {'dry': 0.7, 'damp': 0.2, 'drenched': 0.1})\n",
    "\n",
    "# Second, create a BayesNet object that just stores all the nodes.\n",
    "net = BayesNet([rain_node, sprinklers_node, grass_node, feet_node])\n",
    "\n",
    "# Third, do whatever you want with this data structure, like ask for the conditional distribution for a variable.\n",
    "fetched_node = net.get_node('Feet')\n",
    "assert fetched_node == feet_node  # Just a sanity check\n",
    "# Calculate some joint probabilities\n",
    "joint_prob = net.calc_joint([(rain_node, True), (sprinklers_node, 'off'), (feet_node, 'damp'), (grass_node, 'wet')])\n",
    "print(joint_prob)\n",
    "\n",
    "# Fourth, visualize it all. Right now, visualization is crude (weird layout) but should be correct (arrows the right way.)\n",
    "net.draw_net()\n",
    "\n",
    "# Fifth, show off a fancy new topographical ordering method I just wrote.\n",
    "ordered_nodes = net.get_topographical_ordering()\n",
    "for node in ordered_nodes:\n",
    "    print(node.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Approximate Inference:\n",
    "\n",
    "So far in this notebook, we've stuck with relatively small nets and simple distributions. That means that doing exact inference - calculating analytically exactly what some distribution will look like - is possible. For a lot of problems that we care about, though, exact inference isn't possible. There are lots of reasons this might happen: some distribution is wonky and therefore can't be reasoned about analytically, a net is so complex that doing all the math to marginalize out variables seems impossible, etc.\n",
    "\n",
    "But we don't have to give up. In the following examples, we'll implement two sorts of approximate inference techniques: rejection sampling and Gibbs sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matches_evidence(node, value, evidence):\n",
    "    for evidence_name, evidence_value in evidence:\n",
    "        if node.name == evidence_name:\n",
    "            if value == evidence_value:\n",
    "                return True\n",
    "            return False\n",
    "    return True\n",
    "    \n",
    "# In this sampling method, we just generate random setups through sampling.\n",
    "# If we satisfy the evidence, we save the state; if we don't we reject the sample and keep trying.\n",
    "# After saving lots of states, the hope is that we have enough counts to estimate the frequencies of other variables\n",
    "# conditioned on the evidence.\n",
    "def rejection_sampling(X, e, net, num_samples=10000):\n",
    "    # For each of the variables in X, store a count of how often each value in the domain appears.\n",
    "    # Intialization to zero counts everywhere.\n",
    "    var_to_val_to_count = {}\n",
    "    for x in X:\n",
    "        var_to_val_to_count[x] = {}\n",
    "        for val in net.get_node(x).domain:\n",
    "            var_to_val_to_count[x][val] = 0\n",
    "    # Now, just generate tons of samples in the net, rejecting if it doesn't match the evidence.\n",
    "    num_rejects = 0\n",
    "    for sample_idx in range(num_samples):\n",
    "        # Get a topographical ordering to start sampling.\n",
    "        ordered_nodes = net.get_topographical_ordering()\n",
    "        assignments = {}\n",
    "        reject_sample = False\n",
    "        for node in ordered_nodes:\n",
    "            if node.marginal_distribution:\n",
    "                sample = node.draw_sample()\n",
    "                assignments[node] = sample\n",
    "            else:\n",
    "                parent_val_assignments = [(parent, assignments.get(parent)) for parent in node.parents]\n",
    "                sample = node.draw_sample(parent_vals=parent_val_assignments)\n",
    "                assignments[node] = sample\n",
    "            # Do the rejection part if the node that was sampled contradicts the evidence\n",
    "            if not matches_evidence(node, sample, e):\n",
    "                reject_sample = True\n",
    "                break\n",
    "        if reject_sample:\n",
    "            num_rejects += 1\n",
    "            continue\n",
    "        # Matched the evidence, so update the counts of valid variable assignments\n",
    "        for assigned_node, assigned_val in assignments.items():\n",
    "            if assigned_node.name in var_to_val_to_count.keys():\n",
    "                var_to_val_to_count[assigned_node.name][assigned_val] += 1\n",
    "    # At the end, finally have counts that we can use to compute probabilities.\n",
    "    for x in X:\n",
    "        relevant_counts = var_to_val_to_count.get(x)\n",
    "        total_count = sum([count for count in relevant_counts.values()])\n",
    "        normalized_distribution = {}\n",
    "        for value, count in relevant_counts.items():\n",
    "            normalized_distribution[value] = count / total_count\n",
    "        print(\"Distribution for \", x, \":\", normalized_distribution)\n",
    "    print(\"Num samples used:\", num_samples - num_rejects)\n",
    "    print(\"Num samples rejected\", num_rejects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution for  Feet : {'dry': 0.5102168735113451, 'damp': 0.30036354519242825, 'drenched': 0.18941958129622666}\n",
      "Num samples used: 7977\n",
      "Num samples rejected 2023\n",
      "Distribution for  Rain : {True: 0.2998102466793169, False: 0.7001897533206831}\n",
      "Distribution for  Grass : {'wet': 0.8285895003162556, 'dry': 0.17141049968374447}\n",
      "Num samples used: 1581\n",
      "Num samples rejected 8419\n"
     ]
    }
   ],
   "source": [
    "# Test the rejection sampling code.\n",
    "# First, a really simple example.\n",
    "rejection_sampling(['Feet'], [('Rain', False)], net)\n",
    "# Now, a harder one, with more evidence and asking about more\n",
    "rejection_sampling(['Rain', 'Grass'], [('Sprinklers', 'on'), ('Feet', 'drenched')], net)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous example, we saw how rejection sampling can get us the distributions we want, but we also see one of the big flaws: we end up rejecting lots of samples. Lets do gibbs sampling instead!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXME: this code is ugly, but I do think it's correct.\n",
    "def gibbs_sampling(X, e, net, burn_in_period=100, eval_period=5000):\n",
    "    # For each of the variables in X, store a count of how often each value in the domain appears.\n",
    "    # Intialization to zero counts everywhere.\n",
    "    var_to_val_to_count = {}\n",
    "    for x in X:\n",
    "        var_to_val_to_count[x] = {}\n",
    "        for val in net.get_node(x).domain:\n",
    "            var_to_val_to_count[x][val] = 0\n",
    "    evidence_names = [evidence[0] for evidence in e]\n",
    "    # Intialize the net with random assignments.\n",
    "    assignments = {}\n",
    "    ordered_nodes = net.get_topographical_ordering()\n",
    "    for node in ordered_nodes:\n",
    "        if node.marginal_distribution:\n",
    "            sample = node.draw_sample()\n",
    "            assignments[node] = sample\n",
    "        else:\n",
    "            parent_val_assignments = [(parent, assignments.get(parent)) for parent in node.parents]\n",
    "            sample = node.draw_sample(parent_vals=parent_val_assignments)\n",
    "            assignments[node] = sample\n",
    "    # Now the burn-in section\n",
    "    all_nodes = net.nodes\n",
    "    for trial in range(burn_in_period + eval_period):\n",
    "        # Choose a random node, as long as it's not evidence.\n",
    "        node_to_swap = np.random.choice(all_nodes)\n",
    "        while node_to_swap.name in evidence_names:\n",
    "            node_to_swap = np.random.choice(all_nodes)\n",
    "        # Generate the distribution over next possible values of the node, conditioned on parents and children.\n",
    "        parent_assignments = [(parent, assignments.get(parent)) for parent in node_to_swap.parents]\n",
    "        next_distribution = {}\n",
    "        for next_val in node_to_swap.domain:\n",
    "            prob_given_parents = node_to_swap.get_prob_value(next_val, parent_assignments)\n",
    "            children = net.get_children(node_to_swap)\n",
    "            prob_from_children = 1.0\n",
    "            for child in children:\n",
    "                childs_parent_assignments = []\n",
    "                # But overwrite with next_val\n",
    "                for parent in child.parents:\n",
    "                    if parent == node_to_swap:\n",
    "                        childs_parent_assignments.append((node_to_swap, next_val))\n",
    "                        continue\n",
    "                    childs_parent_assignments.append((parent, assignments.get(parent)))\n",
    "                prob_of_child = child.get_prob_value(assignments.get(child), childs_parent_assignments)\n",
    "                prob_from_children = prob_from_children * prob_of_child\n",
    "            total_prob = prob_given_parents * prob_from_children\n",
    "            next_distribution[next_val] = total_prob\n",
    "        # Normalize the distribution and then sample.\n",
    "        normalizing_factor = sum(next_distribution.values())\n",
    "        for entry, val in next_distribution.items():\n",
    "            next_distribution[entry] = val / normalizing_factor\n",
    "        values = [entry[0] for entry in sorted(next_distribution.items())]\n",
    "        probabilities = [entry[1] for entry in sorted(next_distribution.items())]\n",
    "        sampled = np.random.choice(values, p=probabilities)\n",
    "        assignments[node_to_swap] = sampled\n",
    "        # If after burn-in, start saving data\n",
    "        if trial <= burn_in_period:\n",
    "            continue\n",
    "        # Save the data.\n",
    "        for assigned_node, assigned_val in assignments.items():\n",
    "            if assigned_node.name in var_to_val_to_count.keys():\n",
    "                var_to_val_to_count[assigned_node.name][assigned_val] += 1\n",
    "    # At the end, finally have counts that we can use to compute probabilities.\n",
    "    for x in X:\n",
    "        relevant_counts = var_to_val_to_count.get(x)\n",
    "        total_count = sum([count for count in relevant_counts.values()])\n",
    "        normalized_distribution = {}\n",
    "        for value, count in relevant_counts.items():\n",
    "            normalized_distribution[value] = count / total_count\n",
    "        print(\"Distribution for \", x, \":\", normalized_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution for  Feet : {'dry': 0.5471094218843768, 'damp': 0.27345469093818764, 'drenched': 0.1794358871774355}\n",
      "\n",
      "Distribution for  Rain : {True: 0.27825565113022604, False: 0.7217443488697739}\n",
      "Distribution for  Grass : {'wet': 0.8195639127825565, 'dry': 0.18043608721744347}\n"
     ]
    }
   ],
   "source": [
    "# Test out Gibbs with the same examples as for rejection sampling\n",
    "gibbs_sampling(['Feet'], [('Rain', False)], net)\n",
    "print()\n",
    "gibbs_sampling(['Rain', 'Grass'], [('Sprinklers', 'on'), ('Feet', 'drenched')], net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
