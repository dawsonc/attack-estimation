{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Networks for Estimating Attack Vector\n",
    "\n",
    "## Intro & motivating example\n",
    "\n",
    "Humans are notoriously bad at reasoning about probabilities: more often than not, we rely on cognitive shortcuts (heuristics and biases) instead of actually calculating the liklihood of events. This can be a good thing; just think how paralyzed we would be if we stopped to calculate the probability of rain every time we heard raindrops on our window, rather than just grabbing an umbrella! Even computers can start to run into tractability problems when working with probability distributions involving even a few dozen variables. In this tutorial, we will introduce a tool, called Bayesian networks, for automatically and efficiently reasoning about probabilities. These networks will allow us to ask questions like \"given this new evidence, what is the probability of some hidden variable being true\" or \"what is the most likely explanation for some set of observations?\". These questions might remind you of our lectures on Hidden Markov Models (HMMs); it turns out that Bayesian networks can be used to represent Markov processes, but they are much more expressive and can be used to model much more complicated scenarios.\n",
    "\n",
    "To make this discussion more concrete, it's helpful to have a motivating example. You are a security engineer in the employ of the Generic Galactic Empire aboard a controversial new ~~moon~~ space station. You are aware of several security vulnerabilities in the space station's software subsystems, but you have no way to detect whether those vulnerabilities are being exploited except by running diagnostics on various workstations throughout the station. Since you'd like to be able to detect attacks based on the probability that a vulnerability is being exploited, you'd like an easy way to relate your observations of workstations to the probability of a cyberattack taking place. After a brief review of probability fundamentals, we'll discuss how Bayesian networks can be used to model this situation and make inferences automatically.\n",
    "\n",
    "### References\n",
    "The discussion in this tutorial draws heavily from the following sources:\n",
    "- Russell & Norvig, AIMA, Chapter 13 & 14\n",
    "- [R. Dechter, \"Bucket Elimination: A Unifying Framework for Probabilistic Inference\"](https://webdocs.cs.ualberta.ca/~rgreiner/C-366/RG-2002-SLIDES/BucketElim.pdf) \n",
    "\n",
    "Chapter 13 of AIMA provides an overview of probability fundamentals, while chapter 14 discusses Bayesian networks in more depth. Dechter's article describes efficient algorithms for making inference based on Bayesian networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability Review\n",
    "- This section won't provide a huge overview of probability, but just a reminder of the relevant parts.\n",
    "- Random variables and joint probability distributions (for n binary variables, requires 2^n storage)\n",
    "- Reasoning using joint distributions (13.3)\n",
    "    - just read out e.g. P(A ^ B)\n",
    "    - marginalization to get marginal probability e.g. just P(A) = P(A ^ B) + P(A ^ !B) (plus general eq.)\n",
    "    - Conditional/posterior probabilities, product rule P(A ^ B) = P(A | B)P(B), and conditioning (marginalization but with conditional probs).\n",
    "    - Derive P(X|e) = P(X^e)/P(e) = aP(X^e) = a \\sum_y P(X, e, y) (normalization to get a so that P(X|e)+P(!X|e)=1\n",
    "    - Remark: this would allow us to answer whatever queries we want to ask just by looking in the joint distribution table. BUT that table has size O(2^n) and requires O(2^n) to process by summing over it.\n",
    "- Reasoning using conditional distributions (13.4)\n",
    "    - Lots of redundant information in full joint distribution (like how your toothache affects the weather, ignoring causality)\n",
    "    - Independence! Conditional independence (based on causality)!\n",
    "    - Allows factoring the joint distribution into multiple smaller conditional distributions, saving on size. 2^n+2^m << 2^{n+m}    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Networks\n",
    "\n",
    "Exploiting conditional independence can allow us to reason much more efficiently about joint probabilities, but it would be really nice if we could automate the inference process by encoding conditional probabilities in a convenient data structure. Since dependence relates each variable to a set of \"parent\" variables, a natural structure is a directed graph, where\n",
    "- Each node corresponds to an uncertain variable (in this tutorial, we'll restrict ourselves to discrete random variables),\n",
    "- If variable $B$ is dependent on $A$, then we draw a directed edge from node `A` to node `B` in the graph, and\n",
    "- Each node $X$ is labelled with its conditional probability distribution $P(X\\ |\\ \\text{Parents}(X))$, which takes the form of a table with a row for each combination of assignments to the parent variables of $X$, specifying the probabilities that $X$ takes on each of its values conditioned on those parent variable assignments.\n",
    "\n",
    "Since we don't want any variable to depend (directly or indirectly) on itself, this graph must not have any directed cycles (making it a *directed acyclic graph*. An example graph is shown in the figure below. Given this structure, we can see that it directly encodes both independence and conditional independence: sibling nodes (i.e. nodes sharing a parent) are conditionally independent, while nodes in disconnected graphs are independent.\n",
    "\n",
    "We call this data structure a Bayesian network (or sometimes a *belief network* or *causal network*). These networks can represent any full joint distribution, but depending on the degree to which causal relationships allow us to factor the joint distribution into smaller conditional distributions, Bayesian networks can encode this information much more compactly. In the next example, we'll give a simple example of a Bayesian network and show how we can make inference based on these networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A simple example\n",
    "\n",
    "Before moving on to the more complicated case of our original motivating problem, we'll use the simpler scenario from Fig. 14.2 in Russel & Norvig to illustrate the key features of Bayesian networks. In this scenario, you've just installed a new alarm in your house, and since you don't want Google spying on your home, you've chosen not to connect the alarm to the internet. Instead, you've asked your neighbors, John and Mary, to listen for the alarm and call you if they hear it going off. Complicating matters, the alarm will go off if a burglary occurs, but it is a little too sensitive and will also go off if an earthquake occurs. In addition, John will will sometimes confuse a telephone ringing for the alarm and call then as well, and because Mary likes listening to loud music, she will sometimes miss the alarm altogether.\n",
    "\n",
    "In this scenario, we essentially have five uncertain boolean variables:\n",
    "+ `B`: whether a burglary has occurred,\n",
    "+ `E`: whether an earthquake has occurred,\n",
    "+ `A`: whether the alarm has gone off,\n",
    "+ `J`: whether John calls you, and\n",
    "+ `M`: whether Mary calls you.\n",
    "\n",
    "Note that only `J` and `M` are observable. If we were to encode the full joint probability distribution, we'd need to store a table of size $O(2^n) = O(32)$. This is not too large in thi scase, but if you have even 30 variables then the joint distribution table would require $O(4$ GB$)$ of storage, and adding a 31st variable would require another 4 GB! We are clearly on the wrong side of this exponential when it comes to the joint distribution.\n",
    "\n",
    "To make the problem more tractable, we can exploit the causal relationships between these variables, allowing us to store a handful of smaller conditional distributions rather than a monolithic joint distribution. The causal structure of this problem leads to the Bayesian network shown in the image below (reprinted from Russell & Norvig):\n",
    "\n",
    "<img src=\"files/figs/alarm-problem-structure.png\">\n",
    "\n",
    "Since `J` and `M` only depend on `A`, we don't need to explicitly store the effect of an earthquake on the probability of John calling. Instead of a 32-entry joint distribution, we only need to store 20 values in 5 smaller conditional distributions. Only 10 entries are shown here because for boolean variables the probability of a false value is fixed at 1-(the probability of a true value). For scenarios with even more variables, these savings would be even more dramatic.\n",
    "\n",
    "It's important to note that the Bayesian network representation is not unique: you can order the variables in any way you want and back out the conditional probability tables from the joint distribution. In this case, we have ordered the nodes from causes to effects (causes at the top, flowing down to effects at the bottom of the network). This is a good heuristic for constructing concise Bayesian networks.\n",
    "\n",
    "We've written classes in `bayes_net.py` implementing the basic functionality of Bayesian networks for you, allowing you to represent these networks programmatically. You should go check out the source code to get an idea of what features are available, and a basic example of constructing a network for this example scenario is given below. To properly load the drawing function, **you must run this cell twice**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Have 5 nodes and 4 edges\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Example code for constructing the simple burglary example from AIMA (Fig. 14.2)\n",
    "from bayes_net import BayesNet, BayesNode\n",
    "import numpy as np\n",
    "\n",
    "# First, instantiate the two nodes that have no parents in the network.\n",
    "# The domain of these variables is just {True, False} here, but we support arbitrary discrete domains\n",
    "burglary_node = BayesNode('Burglary')\n",
    "burglary_node.set_marginal_distribution({True: 0.001, False: 0.999})\n",
    "earthquake_node = BayesNode('Earthquake')\n",
    "earthquake_node.set_marginal_distribution({True: 0.002, False: 0.998})\n",
    "\n",
    "# Now we can instantiate nodes with probabilities conditioned on their parents\n",
    "# We have to build the conditional distribution table one entry at a time, for each combination\n",
    "# of parent variables\n",
    "alarm_node = BayesNode('Alarm')\n",
    "alarm_node.add_entry([(burglary_node, True), (earthquake_node, True)], {True: 0.95, False: 0.05})\n",
    "alarm_node.add_entry([(burglary_node, True), (earthquake_node, False)], {True: 0.94, False: 0.06})\n",
    "alarm_node.add_entry([(burglary_node, False), (earthquake_node, True)], {True: 0.29, False: 0.71})\n",
    "alarm_node.add_entry([(burglary_node, False), (earthquake_node, False)], {True: 0.001, False: 0.999})\n",
    "\n",
    "john_node = BayesNode('John')\n",
    "john_node.add_entry([(alarm_node, True)], {True: 0.9, False: 0.1})\n",
    "john_node.add_entry([(alarm_node, False)], {True: 0.05, False: 0.95})\n",
    "\n",
    "mary_node = BayesNode('Mary')\n",
    "mary_node.add_entry([(alarm_node, True)], {True: 0.7, False: 0.3})\n",
    "mary_node.add_entry([(alarm_node, False)], {True: 0.01, False: 0.99})\n",
    "\n",
    "# Now we can create a BayesNet object to store all the nodes.\n",
    "alarm_net = BayesNet([burglary_node, earthquake_node, alarm_node, john_node, mary_node])\n",
    "\n",
    "# As a sanity check, we can visualize the network to make sure it matches our model above.\n",
    "# This visualization is auto-generated, so it will be a bit messy, but we can still see\n",
    "# the overall structure of the network.\n",
    "alarm_net.draw_net()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference on Bayesian Networks\n",
    "\n",
    "Now that we've constructed a bayesian network, what can we do with it? First of all, we can use the network to query the full joint probability distribution as well as asking other questions based on joint probabilities.\n",
    "\n",
    "#### Calculating Joint Probabilities\n",
    "\n",
    "In general, the joint probability for variable assignments $x_1,\\ldots, x_n$ is given by\n",
    "\n",
    "$$P(x_1,\\ldots, x_n) = \\prod_{i=1}^n P(x_i\\ |\\ \\text{Parents}(x_i))$$\n",
    "\n",
    "where we can simply read $P(x_i\\ |\\ \\text{Parents}(x_i)$ out of the conditional distribution attached to node `x_i`. In the context of our specific example, say we want to know the probability `B = True`, `E = False`, `A = True`, `J = True`, `M = True` (the probability that we got a call from John and Mary, the alarm has gone off, there has been a burglary, and there has been no earthquake). Using the general formula above, we get this joint probability as:\n",
    "\n",
    "$$\\begin{align*}\n",
    "P(M, J, A, not E, B) &= P(M | A) \\cdot P(J |A) \\cdot P(A | not E, B) \\cdot P(not E) \\cdot P(B) \\\\\n",
    "                     &= 0.7 \\cdot 0.9 \\cdot 0.94 \\cdot 0.998 \\cdot 0.001 \\\\\n",
    "                     &= 0.00059\n",
    "\\end{align*}$$\n",
    "\n",
    "This functionality is implemented in the provided `BayesNet` class, which we can query directly as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joint_prob = alarm_net.calc_joint([(mary_node, True), \n",
    "                                   (john_node, True),\n",
    "                                   (alarm_node, True),\n",
    "                                   (earthquake_node, False),\n",
    "                                   (burglary_node, True)])\n",
    "print('P(M, J, A, not E, B) = {}'.format(joint_prob))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating Posterior Probabilities\n",
    "\n",
    "With this ability to query the joint distribution, we can answer a broad range of other questions. For instance, if we want to know the posterior probability of some event $X$ after observing some evidence $e$, we can calculate\n",
    "\n",
    "$$P(X|e) = \\alpha P(X, e) = \\alpha \\sum_{y \\notin \\{X,e\\}} P(X, e, y)$$\n",
    "\n",
    "where $\\alpha$ is a normalization constant set so that $P(X|e) + P(not X|e) = 1$. In our example, if we want to know the probability of a burglary having occured if we receive a call from John and Mary, we can calculate\n",
    "\n",
    "$$\\begin{align*}\n",
    "P(B|J, M) = \\alpha P(B, J, M) &= \\alpha \\sum_{a \\in \\{A, not A\\}}\\sum_{e \\in \\{E, notE\\}} P(B, J, M, a, b, e)\n",
    "\\end{align*}$$\n",
    "\n",
    "where $P(B, J, M, a, b, e)$ is a joint probability calculated as shown above. Substituting the equation for joint probabilities into this expression, we get \n",
    "\n",
    "$$\\begin{align*}\n",
    "P(B | J, M) = \\alpha P(B, J, M) &= \\alpha \\sum_{a \\in \\{A, not A\\}} \\sum_{e \\in \\{E, notE\\}} P(B, J, M, a, B, e) \\\\\n",
    "                              &= \\alpha \\sum_{a \\in \\{A, not A\\}}\\sum_{e \\in \\{E, notE\\}} P(M | a) \\cdot P(J | a) \\cdot P(a | e, B) \\cdot P(e) \\cdot P(B)\n",
    "\\end{align*}$$\n",
    "\n",
    "This is a convenient feature, but without further optimization it is pretty inefficient: we need to multiply $n$ values for each of $2^n$ combinations of variable assignments, so this method has time complexity $O(n2^n)$, which again puts us on the losing side of an exponential term. We've implemented this brute-force algorithm (as described in AIMA 14.4.1) in the cell below. You can see how it performs on the simple network we've defined above; later, we'll benchmark the performance of this algorithm compared to more efficient algorithms when run on larger networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def init_distribution(X):\n",
    "    \"\"\"Returns an empty distribution in the form of a nested dictionary, where level i\n",
    "    has keys corresponding to values of variable x_i in X, along with a list of\n",
    "    discrete combinations represented in P\"\"\"\n",
    "    P = dict()\n",
    "    domain_combinations = list(itertools.product(*[list(var.domain) for var in X]))\n",
    "    # this loop constructs a series of nested dictionaries, where each level corresponds\n",
    "    # to a variable in X\n",
    "    for assignments in domain_combinations:\n",
    "        previous_level = P\n",
    "        for val in assignments:\n",
    "            if val not in previous_level:\n",
    "                next_level = dict()\n",
    "                previous_level[val] = next_level\n",
    "                previous_level = next_level\n",
    "            else:\n",
    "                previous_level = previous_level[val]\n",
    "    return P, domain_combinations\n",
    "\n",
    "def update(distribution, assignment, value):\n",
    "    \"\"\"Updates the value for the given assignment in the nested dictionary distribution\"\"\"\n",
    "    head = assignment[0]\n",
    "    if type(distribution[head]) is not dict:\n",
    "        distribution[head] = value\n",
    "    else:\n",
    "        update(distribution[head], assignment[1:], value)\n",
    "\n",
    "def brute_force_query(X, e, net):\n",
    "    \"\"\"Returns the posterior probability P(x | e) using the Bayesian network `net`,\n",
    "    using a brute force enumeration method with time complexity O(n2^n), where\n",
    "    n is the number of variables in the network.\n",
    "    \n",
    "    Inputs:\n",
    "        X: a list of nodes for which we will calculate posteriors\n",
    "        e: a list of tuples (node, value) specifying assignments to evidence variables\n",
    "            (we assume that e and X are disjoint)\n",
    "    Outputs:\n",
    "        P: a distribution over the query variables in X\n",
    "    \"\"\"\n",
    "    # initialize distribution for each value in the domains of the variables in X \n",
    "    P, X_assignments = init_distribution(X)\n",
    "    \n",
    "    # get the list of variables not in X or e\n",
    "    e_vars = [var[0] for var in e]\n",
    "    Y = [node for node in net.nodes if node not in X and node not in e_vars]\n",
    "    \n",
    "    # now we need to calculate the conditional probability for each possible assignment in P\n",
    "    for assignment in X_assignments:\n",
    "        pass\n",
    "    pass\n",
    "\n",
    "X = [burglary_node]\n",
    "e_vars = [john_node, mary_node]\n",
    "P, dc = init_distribution(X)\n",
    "print(json.dumps(P, sort_keys=True, indent=4))\n",
    "net = alarm_net\n",
    "y = [node for node in net.nodes if node not in X and node not in e_vars]\n",
    "\n",
    "print(list(itertools.product(*[list(var.domain) for var in X])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What can they do for you (with reference to the simple example)?\n",
    "    - Answer queries on the joint probability distribution e.g. P(a, b, !c, d, !e) = \\product_i P(x_i | parents(x_i))\n",
    "    - Query the posterior given evidence e.g. P(X | e) = aP(X, e) = a \\sum_{y not x not e} P(X, e, y) <- same as above\n",
    "    - MAP: max liklihood explanation, the X with the highest probability given e."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brute Force Method\n",
    "- Describe brute-force method for getting probability estimates\n",
    "    - 14.4.1\n",
    "    - O(n2^n) for n vars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- benchmark the brute force algorithm to show how exponential it is\n",
    "    - Simple example from last section\n",
    "    - Star wars example from this section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run brute_force_query on the two examples to see how long they take"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Simple speedup via DFS enumeration-ask alg from book fig 14.9\n",
    "    - Explain how it works\n",
    "    - Still super slow: O(2^n) for n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CITE: Russel & Norvig AIMA Ch 14, Section 4, Fig. 14.9\n",
    "def simple_query(X, e, net):\n",
    "    \"\"\"Returns the posterior probability P(x | e) using the Bayesian network `net`,\n",
    "    using an improved enumeration method with time complexity O(2^n), where n is\n",
    "    the number of variables in the network, using analogue to depth-first search.\n",
    "    \n",
    "    Inputs:\n",
    "        X: a list of variable names for which we will calculate posteriors\n",
    "        e: a list of tuples (name, value) specifying assignments to evidence variables\n",
    "            (we assume that e and X are disjoint)\n",
    "    Outputs:\n",
    "        P: a distribution over the query variables in X (a dict mapping names in\n",
    "           X to a probability value)\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "def enumerate_all(variables, e):\n",
    "    \"\"\"Computes one step in the depth-first iteration of the Bayesian network\"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- benchmark the simple algorithm to show how it performs better than the brute force method\n",
    "    - Simple example from last section\n",
    "    - Star wars example from this section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run simple_force_query on the two examples to see how long they take"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A bunch of imports\n",
    "from bayes_net import BayesNet, BayesNode\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A demonstration for developers of how to create a BayesNet. In this example, I define the likelihood of getting\n",
    "# wet feet as a function of wet grass, which is a function of raining and sprinkling.\n",
    "\n",
    "# First, declare a bunch of nodes.\n",
    "rain_node = BayesNode('Rain')\n",
    "rain_node.set_marginal_distribution({True: 0.2, False: 0.8})\n",
    "sprinklers_node = BayesNode('Sprinklers')\n",
    "sprinklers_node.set_marginal_distribution({'on': 0.6, 'off': 0.4})\n",
    "grass_node = BayesNode('Grass')\n",
    "grass_node.add_entry([(rain_node, True), (sprinklers_node, 'on')], {'wet': 0.95, 'dry': 0.05})\n",
    "grass_node.add_entry([(rain_node, True), (sprinklers_node, 'off')], {'wet': 0.6, 'dry': 0.4})\n",
    "grass_node.add_entry([(rain_node, False), (sprinklers_node, 'on')], {'wet': 0.45, 'dry': 0.55})\n",
    "grass_node.add_entry([(rain_node, False), (sprinklers_node, 'off')], {'wet': 0.1, 'dry': 0.90})\n",
    "feet_node = BayesNode('Feet')\n",
    "feet_node.add_entry([(grass_node, 'wet')], {'dry': 0.1, 'damp': 0.5, 'drenched': 0.4})\n",
    "feet_node.add_entry([(grass_node, 'dry')], {'dry': 0.7, 'damp': 0.2, 'drenched': 0.1})\n",
    "\n",
    "# Second, create a BayesNet object that just stores all the nodes.\n",
    "net = BayesNet([rain_node, sprinklers_node, grass_node, feet_node])\n",
    "\n",
    "# Third, do whatever you want with this data structure, like ask for the conditional distribution for a variable.\n",
    "fetched_node = net.get_node('Feet')\n",
    "assert fetched_node == feet_node  # Just a sanity check\n",
    "# Calculate some joint probabilities\n",
    "joint_prob = net.calc_joint([(rain_node, True), (sprinklers_node, 'off'), (feet_node, 'damp'), (grass_node, 'wet')])\n",
    "print(joint_prob)\n",
    "\n",
    "# Fourth, visualize it all. Right now, visualization is crude (weird layout) but should be correct (arrows the right way.)\n",
    "net.draw_net()\n",
    "\n",
    "# Fifth, show off a fancy new topological ordering method I just wrote.\n",
    "ordered_nodes = net.get_topological_ordering()\n",
    "for node in ordered_nodes:\n",
    "    print(node.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approximate Inference:\n",
    "\n",
    "So far in this notebook, we've stuck with relatively small nets and simple distributions. That means that doing exact inference - calculating analytically exactly what some distribution will look like - is possible. For a lot of problems that we care about, though, exact inference isn't possible. There are lots of reasons this might happen: some distribution is wonky and therefore can't be reasoned about analytically, a net is so complex that doing all the math to marginalize out variables seems impossible, etc.\n",
    "\n",
    "But we don't have to give up. In the following examples, we'll implement two sorts of approximate inference techniques: rejection sampling and Gibbs sampling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rejection Sampling:\n",
    "One of the simplest approximate inference algorithms is called rejection sampling. The basic idea is to generate samples from the joint distribution (so, sample all the variables), and then reject any samples that don't match the evidence. By keeping track of the samples that do match the evidence, you can slowly generate examples of what the net looks like when fitting the evidence. In other words, you get a distribution over the joint, conditioned on the evidence, which is exactly what you want.\n",
    "\n",
    "There is a key drawback to rejection sampling, though: if the evidence you are conditioning on is unlikely, you'll have to reject lots of samples before you have a reasonable number of samples you can keep.\n",
    "\n",
    "In the next cell, we implement rejection sampling and produce both the inferred distribution and metrics that track how many samples we had to reject. You'll see that as we condition on rarer events, the percentage of samples we reject increases substantially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_state_tracker(X, net):\n",
    "    var_to_val_to_count = {}\n",
    "    for x in X:\n",
    "        var_to_val_to_count[x] = {}\n",
    "        for val in net.get_node(x).domain:\n",
    "            var_to_val_to_count[x][val] = 0\n",
    "    return var_to_val_to_count\n",
    "\n",
    "# Helper method that establishes if a node being set to a value matches the evidence given.\n",
    "def matches_evidence(node, value, evidence):\n",
    "    for evidence_name, evidence_value in evidence:\n",
    "        if node.name == evidence_name:\n",
    "            if value == evidence_value:\n",
    "                return True\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def compute_dist_from_history(var_to_val_to_count):\n",
    "    for x in var_to_val_to_count.keys():\n",
    "        relevant_counts = var_to_val_to_count.get(x)\n",
    "        total_count = sum([count for count in relevant_counts.values()])\n",
    "        normalized_distribution = {}\n",
    "        for value, count in relevant_counts.items():\n",
    "            normalized_distribution[value] = count / total_count\n",
    "        print(\"Distribution for \", x, \":\", normalized_distribution)\n",
    "    \n",
    "# Implementation of rejection sampling.\n",
    "# X: list of strings of the names of nodes we want to get the distribution of\n",
    "# e: list of tuples of (variable name, assignment) that we use as evidence to condition on\n",
    "# net: the BayesNet object describing the probabilistic relationships between the nodes.\n",
    "# num_samples: integer number of samples to generate. (Note: this is not the number of samples to keep, just to generate.)\n",
    "def rejection_sampling(X, e, net, num_samples=10000):\n",
    "    # For each of the variables in X, store a count of how often each value in the domain appears.\n",
    "    # Intialization to zero counts everywhere.\n",
    "    var_to_val_to_count = initialize_state_tracker(X, net)\n",
    "    \n",
    "    # Now generate samples in the net, rejecting a sample if it doesn't match the evidence.\n",
    "    num_samples_rejected = 0\n",
    "    for sample_idx in range(num_samples):\n",
    "        # Get a topological ordering to start sampling.\n",
    "        ordered_nodes = net.get_topological_ordering()\n",
    "        assignments = {}\n",
    "        reject_sample = False\n",
    "        for node in ordered_nodes:\n",
    "            if node.marginal_distribution:\n",
    "                sample = node.draw_sample()\n",
    "                assignments[node] = sample\n",
    "            else:\n",
    "                parent_val_assignments = [(parent, assignments.get(parent)) for parent in node.parents]\n",
    "                sample = node.draw_sample(parent_vals=parent_val_assignments)\n",
    "                assignments[node] = sample\n",
    "            # Reject if the node that was sampled contradicts the evidence\n",
    "            if not matches_evidence(node, sample, e):\n",
    "                reject_sample = True\n",
    "                break  # No point in continuing to sample further nodes if one already doesn't match e.\n",
    "        if reject_sample:\n",
    "            num_samples_rejected += 1\n",
    "            continue\n",
    "        # Matched the evidence, so update the counts of valid variable assignments\n",
    "        for assigned_node, assigned_val in assignments.items():\n",
    "            if assigned_node.name in var_to_val_to_count.keys():\n",
    "                var_to_val_to_count[assigned_node.name][assigned_val] += 1\n",
    "    # We have the counts that we can use to compute probabilities, so do the final synthesis.\n",
    "    compute_dist_from_history(var_to_val_to_count)\n",
    "    # And print out metrics about how many samples were rejected\n",
    "    print(\"Percentage of samples rejected\", num_samples_rejected / num_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the rejection sampling code.\n",
    "# First, a really simple example.\n",
    "rejection_sampling(['Feet'], [('Rain', False)], net)\n",
    "# Now, a harder one, with evidence that is less likely. The percentage of rejections increases substantially.\n",
    "rejection_sampling(['Rain', 'Grass'], [('Sprinklers', 'on'), ('Feet', 'drenched')], net)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gibbs Sampling\n",
    "Now that we have a handle on rejection sampling, let's see if we can use some of the same ideas (sampling instead of exact calculations) without paying the cost of a huge number of samples that we discard.\n",
    "\n",
    "In Gibbs sampling, the basic idea is to form a Markov Chain (MC) over the joint states. In one timestep, the MC will have a full assignment. In the next, exactly one of the variables in the net will be randomly sampled, conditioned on its Markov blanket. That means that either zero or one variables changes at each timestep.\n",
    "\n",
    "It turns out (and AIMA explains the theory in far more depth), that if you run an MC like this for a long time, assuming some nice properties of the Bayes Net, the distribution of joint states visited in the MC exactly matches the joint distribution of the net. Furthermore, conditioning on variables being assigned to specific values becomes extremely easy: when transitioning to the next state in the MC, never change the evidence variables from their assigned values. That means that every single state the MC visits will match the evidence, neatly fixing the issue with rejection sampling's inefficiency.\n",
    "\n",
    "Unfortunately, Gibbs sampling isn't just a pure win. How one intializes all the variables to start of the MC can matter, at least in the short term. Imagine if the net were initialized to some extremely unlikely configuration and the MC were only run for a short time. The empirical distribution would likely not reflect the true distribution. The good news is that, over time, the effects of the initialization should wear off. That's why many researchers use what's called a \"burn in\" period - some number of samples where you run the MC after initialization but without recording any of the data. How long is long enough for burn in? That's an open research question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_assignments(net):\n",
    "    assignments = {}\n",
    "    ordered_nodes = net.get_topological_ordering()\n",
    "    for node in ordered_nodes:\n",
    "        if node.marginal_distribution:\n",
    "            sample = node.draw_sample()\n",
    "            assignments[node] = sample\n",
    "        else:\n",
    "            parent_val_assignments = [(parent, assignments.get(parent)) for parent in node.parents]\n",
    "            sample = node.draw_sample(parent_vals=parent_val_assignments)\n",
    "            assignments[node] = sample\n",
    "    return assignments\n",
    "\n",
    "def gibbs_sampling(X, e, net, burn_in_period=100, eval_period=5000):\n",
    "    # For each of the variables in X, store a count of how often each value in the domain appears.\n",
    "    var_to_val_to_count = initialize_state_tracker(X, net)\n",
    "    \n",
    "    evidence_names = [evidence[0] for evidence in e]\n",
    "    \n",
    "    # Intialize the net with random assignments.\n",
    "    assignments = initialize_assignments(net)\n",
    "    \n",
    "    # Run the MC.\n",
    "    all_nodes = net.nodes\n",
    "    for trial in range(burn_in_period + eval_period):\n",
    "        # Choose a random node, as long as it's not evidence.\n",
    "        node_to_swap = np.random.choice(all_nodes)\n",
    "        while node_to_swap.name in evidence_names:\n",
    "            node_to_swap = np.random.choice(all_nodes)\n",
    "        # Generate the distribution over next possible values of the node, conditioned on the Markov blanket of the node.\n",
    "        parent_assignments = [(parent, assignments.get(parent)) for parent in node_to_swap.parents]\n",
    "        children = net.get_children(node_to_swap)\n",
    "        \n",
    "        # For each possible next value the node could take, find the likelihood, conditioned on the\n",
    "        # Markov blanket.\n",
    "        next_distribution = {}\n",
    "        for next_val in node_to_swap.domain:\n",
    "            prob_given_parents = node_to_swap.get_prob_value(next_val, parent_assignments)\n",
    "            # Calculate the probability of the children's values given the next assignment\n",
    "            prob_of_children = 1.0\n",
    "            for child in children:\n",
    "                childs_parent_assignments = []\n",
    "                # For the child's parents, take the existing values except for the value for the current node,\n",
    "                # which must be replaced with next_val.\n",
    "                for parent in child.parents:\n",
    "                    if parent == node_to_swap:\n",
    "                        childs_parent_assignments.append((node_to_swap, next_val))\n",
    "                        continue\n",
    "                    childs_parent_assignments.append((parent, assignments.get(parent)))\n",
    "                prob_of_child = child.get_prob_value(assignments.get(child), childs_parent_assignments)\n",
    "                prob_of_children = prob_of_children * prob_of_child\n",
    "            total_prob = prob_given_parents * prob_of_children\n",
    "            next_distribution[next_val] = total_prob\n",
    "        # Normalize the distribution.\n",
    "        normalizing_factor = sum(next_distribution.values())\n",
    "        for entry, val in next_distribution.items():\n",
    "            next_distribution[entry] = val / normalizing_factor\n",
    "        # Sample from the distribution to make the assignment.\n",
    "        values = [entry[0] for entry in sorted(next_distribution.items())]\n",
    "        probabilities = [entry[1] for entry in sorted(next_distribution.items())]\n",
    "        sampled = np.random.choice(values, p=probabilities)\n",
    "        assignments[node_to_swap] = sampled\n",
    "        # If after burn-in, start saving data\n",
    "        if trial <= burn_in_period:\n",
    "            continue\n",
    "        # Save the data.\n",
    "        for assigned_node, assigned_val in assignments.items():\n",
    "            if assigned_node.name in var_to_val_to_count.keys():\n",
    "                var_to_val_to_count[assigned_node.name][assigned_val] += 1\n",
    "\n",
    "    compute_dist_from_history(var_to_val_to_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test out Gibbs with the same examples as for rejection sampling\n",
    "gibbs_sampling(['Feet'], [('Rain', False)], net)\n",
    "print()\n",
    "gibbs_sampling(['Rain', 'Grass'], [('Sprinklers', 'on'), ('Feet', 'drenched')], net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
